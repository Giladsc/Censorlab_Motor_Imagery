{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some standard pythonic imports\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import logging\n",
    "import os,numpy as np,pandas as pd\n",
    "from collections import OrderedDict\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "# MNE functions\n",
    "import mne\n",
    "from mne import Epochs,find_events\n",
    "from mne.decoding import Vectorizer\n",
    "\n",
    "#MNE XDF Importer\n",
    "from mne_import_xdf import *\n",
    "\n",
    "# Scikit-learn and Pyriemann ML functionalities\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import ShuffleSplit, cross_val_score,train_test_split\n",
    "from sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, StratifiedShuffleSplit, train_test_split\n",
    "from pyriemann.estimation import ERPCovariances, XdawnCovariances, Xdawn, Covariances\n",
    "from pyriemann.tangentspace import TangentSpace\n",
    "from pyriemann.classification import MDM\n",
    "import mne\n",
    "\n",
    "from mne.io import concatenate_raws, read_raw_edf\n",
    "from mne.datasets import eegbci\n",
    "from mne.decoding import CSP\n",
    "\n",
    "\n",
    "\n",
    "import pyxdf\n",
    "import PyQt5\n",
    "\n",
    "from easygui import *\n",
    "\n",
    "import pathlib\n",
    "\n",
    "# For interactive plots\n",
    "from IPython import get_ipython\n",
    "get_ipython().run_line_magic('matplotlib', 'qt')\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import json\n",
    "\n",
    "#import moab to get the filterbank implementation: \n",
    "from moabb.pipelines.utils import FilterBank\n",
    "\n",
    "#imports for precision_recall_curve related plot: \n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import average_precision_score, precision_recall_curve,PrecisionRecallDisplay\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from itertools import cycle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all available recording files ['Gilad_AO.xdf', 'G_BCI_No_AO.xdf', 'Neta_AO_1Hand.xdf', 'Neta_AO_2Hands.xdf', 'Neta_NoAO_1Hand.xdf', 'Neta_NoAO_2Hands.xdf', 'NH_Block_1.xdf', 'NH_Block_2.xdf', 'NH_Block_3.xdf', 'Ro555ei_P300.xdf', 'Ro555ei_P300_1_24_05.xdf', 'Ron-Block_1.xdf', 'Ron-Block_2.xdf', 'Ron-Block_3.xdf', 'Ron-Block_4.xdf', 'sub-Roei_ses-MI1_task-Default_run-001_eeg.xdf', 'sub-Roei_ses-MI2_task-Default_run-001_eeg.xdf', 'sub-Roei_ses-Mi3_task-Default_run-001_eeg.xdf']\n",
      "only subjects IDS: ['Gilad', 'G', 'Neta', 'Neta', 'Neta', 'Neta', 'NH', 'NH', 'NH', 'Ro555ei', 'Ro555ei', 'Ron-Block', 'Ron-Block', 'Ron-Block', 'Ron-Block', 'sub-Roei', 'sub-Roei', 'sub-Roei']\n"
     ]
    }
   ],
   "source": [
    "#define paths: \n",
    "current_path = pathlib.Path().absolute()  \n",
    "recording_path = current_path / 'Recordings'\n",
    "figure_outputs_path=current_path / 'figures_outputs'\n",
    "hyper_param_search_output=current_path / 'hyper_param_search_outputs'\n",
    "#extract all recorded files and subject names\n",
    "recording_files = [f for f in listdir(recording_path) if isfile(join(recording_path, f)) and ('.xdf' in f)]\n",
    "if not(figure_outputs_path.exists()):\n",
    "    print('the output folder does not exists:  ',figure_outputs_path)\n",
    "\n",
    "if not(hyper_param_search_output.exists()):\n",
    "    print('the output folder does not exists:  ',hyper_param_search_output)\n",
    "\n",
    "\n",
    "print('all available recording files',recording_files)\n",
    "subject_names=[r.split('_')[0] for r in recording_files]\n",
    "print('only subjects IDS:',subject_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filenames:\n",
      " ['Gilad_AO.xdf', 'G_BCI_No_AO.xdf', 'Neta_AO_1Hand.xdf', 'Neta_AO_2Hands.xdf', 'Neta_NoAO_1Hand.xdf', 'Neta_NoAO_2Hands.xdf', 'NH_Block_1.xdf', 'NH_Block_2.xdf', 'NH_Block_3.xdf', 'Ro555ei_P300.xdf', 'Ro555ei_P300_1_24_05.xdf', 'Ron-Block_1.xdf', 'Ron-Block_2.xdf', 'Ron-Block_3.xdf', 'Ron-Block_4.xdf', 'sub-Roei_ses-MI1_task-Default_run-001_eeg.xdf', 'sub-Roei_ses-MI2_task-Default_run-001_eeg.xdf', 'sub-Roei_ses-Mi3_task-Default_run-001_eeg.xdf']\n",
      "names:\n",
      " ['Gilad', 'G', 'Neta', 'Neta', 'Neta', 'Neta', 'NH', 'NH', 'NH', 'Ro555ei', 'Ro555ei', 'Ron-Block', 'Ron-Block', 'Ron-Block', 'Ron-Block', 'sub-Roei', 'sub-Roei', 'sub-Roei']\n",
      "grid options [[0], [0, 1], [0, 1, 2], [0], [0, 1], [0], [0, 1], [0], [0], [0, 1], [0, 1], [0, 1, 2]]\n",
      "number of grid search iterations: 288\n",
      "Grid info: {'filter_methods': ['iir'], 'run_csd': [True, False], 'pipeline_name': ['csp+lda', 'ts+lda', 'fbcsp+lda'], 'bandpass_borders_grid': [[7, 32]], 'Electorde_Groups_names_grid': ['C', 'PO'], 'n_components_grid': [4], 'n_components_fbcsp_grid': [2, 3], 'filters_bands': [[8, 12]], 'epoch_tmins_and_maxes_grid': [[-3, 4]], 'classifier_training_windows_grid': [[1, 2], [1, 3]], 'augmentation_windows_grid': [[0, 0], [0.5, 0.5]], 'windowed_prediction_params': [[0.5, 0.1], [0.75, 0.1], [1, 0.1]]}\n"
     ]
    }
   ],
   "source": [
    "#initial defitions: \n",
    "print('filenames:\\n',recording_files)\n",
    "print('names:\\n',subject_names)\n",
    "\n",
    "Use_test_grid=False #change to False when you want to use the real grid_search and not a toy one: \n",
    "\n",
    "#define the electdode groups: the key can be anything, the values should be a list of electrodes\n",
    "Electorde_Groups = {'FP': ['Fp1', 'Fp2'],\n",
    "                   'AF': ['AF7', 'AF3', 'AFz', 'AF4', 'AF8'],\n",
    "                   'F' : ['F7', 'F5', 'F3', 'F1', 'Fz', 'F2', 'F4', 'F6', 'F8'],\n",
    "                   'FC': ['FC5', 'FC3', 'FC1', 'FC2', 'FC4', 'FC6', 'FT7','FT8'],\n",
    "                   'C' : ['C5', 'C3', 'C1', 'Cz', 'C2', 'C4' ,'C6'],\n",
    "                   'CP': ['CP5', 'CP3', 'CP1','CPz', 'CP2', 'CP4', 'CP6','TP7', 'TP8'],\n",
    "                   'P' : ['P7', 'P5','P3', 'P1', 'Pz', 'P2', 'P4', 'P6', 'P8'],\n",
    "                   'PO': ['PO7', 'PO3', 'POz', 'PO4', 'PO8'],\n",
    "                   'O' : ['Oz', 'O2', 'O1', 'Iz']\n",
    "                  } \n",
    "\n",
    "#define the grid search (dont go all at once because some params are not relevant to other params and might just increase running time: \n",
    "# i.e if using fbcsp, the n_components_grid paramater is not used, so if it has more than 1 value, it will run the fbcsp twice while changing a paramter that does not effect the calculation)\n",
    "grid_search_dict=OrderedDict()\n",
    "grid_search_dict={'filter_methods':['iir'], #['irr' or 'fir']\n",
    "                'run_csd':[True, False],\n",
    "                'pipeline_name':['csp+lda','ts+lda','fbcsp+lda'], #these classifiers pipelines are defined in \"run_windowed_classification_on_fold\"\n",
    "                #things to do: filter bank csp + lda, csp+ts+lda\n",
    "                'bandpass_borders_grid':[[7,32]], #each list defines the low and high cutoffs\n",
    "                'Electorde_Groups_names_grid':['C','PO'], #each \"name\" refers to an elec group defined above\n",
    "                'n_components_grid':[4], #the n component options for the csp classifier\n",
    "                'n_components_fbcsp_grid':[2,3], # the n components options to use in the fbcsp classifier (n * filter_bank_bands)\\\n",
    "                'filters_bands':[[8,12]],#[[[8, 12], [12, 20], [20, 32]]],\n",
    "                'epoch_tmins_and_maxes_grid':[[-3,4]], #times (sec: pre,post) for initial epoching (this should be the longest epoch as the windowed prediction will be tested on it)\n",
    "                'classifier_training_windows_grid':[[1 , 2],[1,3]], #what times(sec: start,end) to use for the classifer training (data augmentation is also using this window)\n",
    "                'augmentation_windows_grid':[[0,0],[0.5,0.5]], #referes to proportions (win_len,win_step) of sfreq, [1,1] means taking the classification epochs, and creating 1 second long epochs with 1 second long steps\n",
    "                'windowed_prediction_params':[[0.5,0.1],[0.75,0.1],[1,0.1]]} #refers to prportions (win_len,win_step) of sfreq to try and predict i.e. 0.5 = half a second window, with a 100ms steps  \n",
    "\n",
    "#here you can define a test grid (make it small so it wont take long, and use it to check that everything is working) \n",
    "test_grid_search_dict={'filter_methods':['fir'], #['irr' or 'fir']\n",
    "                'run_csd':[True],\n",
    "                'pipeline_name':['csp+lda'], #these classifiers pipelines are defined in \"run_windowed_classification_on_fold\n",
    "                #things to do: filter bank csp + lda, csp+ts+lda\n",
    "                'bandpass_borders_grid':[[8,32]], #each list defines the low and high cutoffs\n",
    "                'Electorde_Groups_names_grid':['C','PO'], #each \"name\" refers to an elec group defined above\n",
    "                'n_components_grid':[4], #the n component options for the csp classifier\n",
    "                'n_components_fbcsp_grid':[3], # the n components options to use in the fbcsp classifier (n * filter_bank_bands)\n",
    "                'filters_bands':[[[8, 12], [12, 20], [20, 32]]],\n",
    "                'epoch_tmins_and_maxes_grid':[[-3,5]], #times (sec: pre,post) for initial epoching (this should be the longest epoch as the windowed prediction will be tested on it)\n",
    "                'classifier_training_windows_grid':[[0,2]], #what times(sec: start,end) to use for the classifer training (data augmentation is also using this window)\n",
    "                'augmentation_windows_grid':[[1,0.1]], #referes to proportions (win_len,win_step) of sfreq, [1,1] means taking the classification epochs, and creating 1 second long epochs with 1 second long steps\n",
    "                'windowed_prediction_params':[[1,0.1]]} #refers to prportions (win_len,win_step) of sfreq to try and predict i.e. 0.5 = half a second window, with a 100ms steps  \n",
    "\n",
    "if Use_test_grid: \n",
    "   print('\\n######\\nusing a test grid search\\n######\\n')\n",
    "   grid_search_dict=test_grid_search_dict\n",
    "\n",
    "all_options=[list(range(len(val))) for key,val in grid_search_dict.items()]\n",
    "print(f'grid options {all_options}')\n",
    "#get all possible grid_search combinations: \n",
    "all_grid_combinations = list(itertools.product(*all_options))\n",
    "print(f'number of grid search iterations: {len(all_grid_combinations)}')\n",
    "print('Grid info:',grid_search_dict)\n",
    "#save the hyper_grid_search: \n",
    "with open(hyper_param_search_output/'grid_search_info.json', 'w') as file:\n",
    "    json.dump(grid_search_dict, file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define trigger and bad electrodes information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the trigger info per participant\n",
    "#new subjects should be added here, untill we decide on a single trigger... \n",
    "def define_events_trigger_values_per_recording_file(recording_file):\n",
    "    events_dictionary={'Dekel_AO.xdf':{3:'left',6:'right'},\n",
    "                    'Dekel_AoNoMI.xdf':{3:'left',6:'right'},\n",
    "                    'Dekel_NoAO.xdf':{3:'left',6:'right'}, \n",
    "                    'Gilad_AO.xdf':{4:'left',7:'right'}, \n",
    "                    'Neta_AO_1Hand.xdf':{2:'close',5:'open'},\n",
    "                    'Neta_AO_2Hands.xdf':{3:'left',6:'right'},\n",
    "                    'Neta_NoAO_1Hand.xdf':{2:'close',5:'open'},\n",
    "                    'Neta_NoAO_2Hands.xdf':{3:'left',6:'right'},\n",
    "                    'Ron-Block_2.xdf':{3:'left',6:'rest',7:'right'},\n",
    "                    'Ron-Block_3.xdf':{3:'left',6:'rest',7:'right'},\n",
    "                    'Ron-Block_4.xdf':{3:'left',6:'rest',7:'right'},\n",
    "                    'sub-Roei_ses-MI1_task-Default_run-001_eeg.xdf': {2:'left',5:'rest',6:'right'}, \n",
    "                    'sub-Roei_ses-MI2_task-Default_run-001_eeg.xdf': {3:'left',6:'rest',7:'right'},  \n",
    "                    'sub-Roei_ses-Mi3_task-Default_run-001_eeg.xdf': {3:'left',6:'rest',7:'right'},\n",
    "                    'NH_Block_1.xdf': {3:'left',6:'rest',7:'right'},\n",
    "                    'NH_Block_2.xdf': {3:'left',6:'rest',7:'right'},\n",
    "                    'NH_Block_3.xdf': {3:'left',6:'rest',7:'right'}}\n",
    "    if recording_file not in events_dictionary.keys():\n",
    "        raise Exception(f'the current recording file {recording_file} does not have a defined triggers in the function \"define_events_triggers_Values_per_recorindg_file')\n",
    "    \n",
    "    trigger_info_dict=events_dictionary[recording_file]\n",
    "    return trigger_info_dict\n",
    "    \n",
    "\n",
    "#define what electdodes should be excluded (based on whatever method you'd like (probably visual inspection of the recordings))\n",
    "def get_subject_bad_electrodes(subject):\n",
    "    elecs_to_drop={}\n",
    "    #define here the subject specific electdodes to make sure are removed from the data: \n",
    "    bad_elecs_dict={'Dekel':{'FT10', 'TP10', 'FT9'},\n",
    "                    'Gilad':{'FT10', 'TP10', 'FT9', 'TP9'},\n",
    "                    'Neta':{'TP9'},\n",
    "                    'Ron-Block':{'PO7'},\n",
    "                    'sub-Roei': {'TP9'},\n",
    "                    'NH' : {'TP7'}}\n",
    "    if subject in bad_elecs_dict.keys():\n",
    "        subject_bad_electrodes=bad_elecs_dict[subject]\n",
    "    else: \n",
    "        subject_bad_electrodes={}\n",
    "        print('note that no bad electrodes were defined for the current subject:',subject)\n",
    "    return subject_bad_electrodes   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_up_params_for_current_grid_iteration(all_grid_combinations,iteration_ind,grid_search_dict):\n",
    "    curr_grid_comb=all_grid_combinations[iteration_ind]\n",
    "    print(f'setting up current params: iteration {iteration_ind} - grid settings: {curr_grid_comb}')\n",
    "    #create a dictionary from the current grid combination (note that the dictionary is ordered): \n",
    "    iteration_dictionary={key:val[inner_ind] for ((key,val),inner_ind) in zip(grid_search_dict.items(),curr_grid_comb)}\n",
    "   \n",
    "    #extract the paramaters for the current iteration: \n",
    "    LowPass=iteration_dictionary['bandpass_borders_grid'][0]\n",
    "    HighPass=iteration_dictionary['bandpass_borders_grid'][1]\n",
    "    PerformCsd=iteration_dictionary['run_csd']\n",
    "    filter_method=iteration_dictionary['filter_methods']\n",
    "\n",
    "    #extract current electrodes (allow for combination of electrode groups i.e 'C+AF+F')\n",
    "    Electorde_Group_name=iteration_dictionary['Electorde_Groups_names_grid'] #['FP', 'AF', 'F', 'FC', 'C', 'CP', 'P', 'PO', 'O']\n",
    "    Electorde_Group=[]\n",
    "    for cur_elec_group_name in Electorde_Group_name.split('+'):\n",
    "        Electorde_Group=Electorde_Group+Electorde_Groups[cur_elec_group_name]\n",
    "    classifier_window_s=iteration_dictionary['classifier_training_windows_grid'][0]\n",
    "    classifier_window_e=iteration_dictionary['classifier_training_windows_grid'][1]\n",
    "    epoch_tmin=iteration_dictionary['epoch_tmins_and_maxes_grid'][0]\n",
    "    epoch_tmax=iteration_dictionary['epoch_tmins_and_maxes_grid'][1]\n",
    "    n_components=iteration_dictionary['n_components_grid']\n",
    "    n_components_fbcsp=iteration_dictionary['n_components_fbcsp_grid']\n",
    "    #define the current augmentation paramaters to test: note that they are defined by samples\n",
    "    augmentation_params={'win_len':iteration_dictionary['augmentation_windows_grid'][0],\n",
    "                        'win_step':iteration_dictionary['augmentation_windows_grid'][1]}\n",
    "    #define the windowed prediction paramaters: #here they are defined as proportions of the sampling frequency\n",
    "    windowed_prediction_params={'win_len':iteration_dictionary['windowed_prediction_params'][0],\n",
    "                                'win_step':iteration_dictionary['windowed_prediction_params'][1]}\n",
    "    #get the current pipeline name: \n",
    "    pipeline_name=iteration_dictionary['pipeline_name']\n",
    "    filters_bands=iteration_dictionary['filters_bands']                          \n",
    "    #set paramaters dict for current run: \n",
    "    params_dict={'LowPass': LowPass,\n",
    "                'HighPass': HighPass,\n",
    "                'PerformCsd':PerformCsd,\n",
    "                'filter_method':filter_method,\n",
    "                'n_components':n_components,\n",
    "                'n_components_fbcsp':n_components_fbcsp,\n",
    "                'filters_bands':filters_bands,\n",
    "                'Electorde_Group':Electorde_Group,\n",
    "                'Electorde_Group_name':Electorde_Group_name,\n",
    "                'epoch_tmin':epoch_tmin,\n",
    "                'epoch_tmax':epoch_tmax,\n",
    "                'classifier_window_s':classifier_window_s,\n",
    "                'classifier_window_e':classifier_window_e,\n",
    "                'augmentation_params':augmentation_params,\n",
    "                'windowed_prediction_params':windowed_prediction_params,\n",
    "                'pipeline_name':pipeline_name}\n",
    "\n",
    "    return params_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to define a preprocessing function that can accept several participant files and add them to a single structure. \n",
    "def run_pre_processing_extract_validation_set(recording_path,current_path,params_dict):\n",
    "\n",
    "    #extract the current run paramaters: \n",
    "    Subject=params_dict['subject']\n",
    "    PerformCsd=params_dict['PerformCsd']\n",
    "    LowPass, HighPass, filter_method = params_dict['LowPass'],params_dict['HighPass'],params_dict['filter_method']\n",
    "    tmin=params_dict['epoch_tmin']\n",
    "    tmax=params_dict['epoch_tmax']\n",
    "\n",
    "    events_trigger_dict=define_events_trigger_values_per_recording_file(params_dict['recording_file'])\n",
    "    events_trigger_dict={val:key for key,val in events_trigger_dict.items()}\n",
    "\n",
    "    #read the file:\n",
    "    Raw=read_raw_xdf(recording_path / params_dict['recording_file'])\n",
    "    #remove non existent channels: \n",
    "    Raw.drop_channels(['ACC_X','ACC_Y','ACC_Z']) ## Drop non eeg channels\n",
    "    #set the correct (Brainvision Montage) montage:\n",
    "    montage = mne.channels.read_custom_montage((f\"{current_path}\\Montages\\CACS-64_REF.bvef\"), head_size=0.095, coord_frame=None) \n",
    "    #rename channels for consistency: \n",
    "    mne.rename_channels(Raw.info, {'F9' : 'FT9','P9' : 'TP9','P10' : 'TP10','F10' : 'FT10','AF1' : 'AF7' }, allow_duplicates=False, verbose=None)\n",
    "    Raw.set_montage(montage, match_case=True, match_alias=False, on_missing='raise', verbose=None)\n",
    "\n",
    "    print('\\n###########################################################')\n",
    "    print('removing subject sepecific bad electrodes from the raw data')\n",
    "    #drop bad electrodes according to the current subject name: \n",
    "    print('\\n###########################################################')\n",
    "    print('removing bad channels from epochs:')\n",
    "    curr_elecs_in_epochs_set=set(Raw.info['ch_names'])\n",
    "    elecs_to_remove=get_subject_bad_electrodes(Subject)\n",
    "    elecs_to_drop=curr_elecs_in_epochs_set.intersection(elecs_to_remove)\n",
    "\n",
    "    if len(elecs_to_drop)>0: \n",
    "        Raw.drop_channels(list(elecs_to_drop))\n",
    "    Raw.drop_channels(Raw.info['bads'])\n",
    "\n",
    "    #do csd: \n",
    "    if (PerformCsd):\n",
    "        print('\\n###########################################################')\n",
    "        print('running csd')\n",
    "        Raw_CSD = mne.preprocessing.compute_current_source_density(Raw) ## Compute CSD\n",
    "    else :\n",
    "        print('\\n###########################################################')\n",
    "        print('not using csd')\n",
    "        Raw_CSD = Raw\n",
    "\n",
    "    print('\\n###########################################################')\n",
    "    print('filtering the data')  \n",
    "    unfiltered_Raw_CSD=Raw_CSD.copy()\n",
    "    Raw_CSD_Filtered = unfiltered_Raw_CSD.filter(LowPass, HighPass, method=filter_method)\n",
    "\n",
    "    #extract filterbank feequencies:\n",
    "    filters_bands=tuple(params_dict['filters_bands'])\n",
    "    filtered_data_band_passed=[]\n",
    "    for i,(LowPass,HighPass) in enumerate(filters_bands):\n",
    "        unfiltered_Raw_CSD=Raw_CSD.copy()\n",
    "        Raw_CSD_Filtered_band= unfiltered_Raw_CSD.filter(LowPass, HighPass, method=filter_method)\n",
    "        filtered_data_band_passed.append(Raw_CSD_Filtered_band)\n",
    "\n",
    "    events_from_annot,event_dict = mne.events_from_annotations(Raw_CSD_Filtered)\n",
    "    print('\\n###########################################################')\n",
    "    print('extracting event info:',event_dict)\n",
    "\n",
    "    print('\\n###########################################################')\n",
    "    selected_elecs=params_dict['Electorde_Group']\n",
    "\n",
    "    #filter bank related: \n",
    "    filter_bank_epochs=[]\n",
    "    for filtered_data_band in filtered_data_band_passed:\n",
    "        epochs = mne.Epochs(filtered_data_band, events_from_annot,picks=params_dict['Electorde_Group'], preload = True,baseline= None, tmin=tmin, tmax=tmax, event_id=events_trigger_dict,detrend=0)\n",
    "        filter_bank_epochs.append(epochs)\n",
    "\n",
    "    print(f'epoching + selecting current electodes set for analysis:\\n{selected_elecs}')\n",
    "    epochs = mne.Epochs(Raw_CSD_Filtered, events_from_annot,picks=params_dict['Electorde_Group'], preload = True,baseline= None, tmin=tmin, tmax=tmax, event_id=events_trigger_dict,detrend=0)\n",
    "\n",
    "\n",
    "    #this section drops electrodes after epoching: but currently we drop all bad electrodes from the raw data\n",
    "    print('\\n###########################################################')\n",
    "    print('removing bad channels from epochs:')\n",
    "    curr_elecs_in_epochs_set=set(epochs.info['ch_names'])\n",
    "    elecs_to_remove=get_subject_bad_electrodes(Subject)\n",
    "    elecs_to_drop=curr_elecs_in_epochs_set.intersection(elecs_to_remove)\n",
    "\n",
    "    if len(elecs_to_drop)>0:\n",
    "        epochs.info['bads']=elecs_to_drop\n",
    "        epochs.drop_channels(epochs.info['bads'])\n",
    "        print('\\n###########################################################')\n",
    "        print(f'Removed: {elecs_to_drop} from the current selected electrodes: {curr_elecs_in_epochs_set} from the overall set of bad electrodes {elecs_to_remove}')\n",
    "        print('#############################################################')\n",
    "    \n",
    "        #filter bank related: \n",
    "        filter_bank_epochs_after_elec_drops=[]\n",
    "        for curr_epochs in filter_bank_epochs:\n",
    "            curr_epochs.info['bads']=elecs_to_drop\n",
    "            curr_epochs.drop_channels(epochs.info['bads'])\n",
    "            filter_bank_epochs_after_elec_drops.append(curr_epochs)    \n",
    "    else: \n",
    "        #filter bank related: \n",
    "        filter_bank_epochs_after_elec_drops=[]\n",
    "        for curr_epochs in filter_bank_epochs:\n",
    "            filter_bank_epochs_after_elec_drops.append(curr_epochs)  \n",
    "\n",
    "        print('\\n###########################################################')\n",
    "        print(f'the current selected electrodes: {curr_elecs_in_epochs_set} allready exclude the requested electrodes to remove {elecs_to_remove}')\n",
    "        print('#############################################################')\n",
    "\n",
    "    #extract the validation set: we will use it only after selecting all hyper paramaters to get a better representation of out-of-sample performence: \n",
    "    #using a very small test size as we currently mostly look at the CV scores: \n",
    "    data_df=pd.DataFrame(data=epochs.events[:, -1], columns=['label'] ,index=range(len(epochs.events[:, -1])))\n",
    "    data_df['original_trial_ind']=range(len(epochs.events[:, -1]))\n",
    "    train,validation=train_test_split(data_df,shuffle=True,random_state=42,stratify=data_df['label'],test_size=0.2)\n",
    " \n",
    "    train_inds=train['original_trial_ind'].values\n",
    "    validation_inds=validation['original_trial_ind'].values\n",
    "    print(f'putting aside 20% of the data: trial numbers are:\\n {validation_inds}\\n')\n",
    "    print(f'remaining 80% of the trials go into training for cv:\\n {train_inds}\\n')\n",
    "\n",
    "    return_dict={'train_inds':train_inds,\n",
    "                'validation_inds':validation,\n",
    "                'epochs':epochs,\n",
    "                'filter_bank_epochs':filter_bank_epochs_after_elec_drops,\n",
    "                'events_triggers_dict':events_trigger_dict}\n",
    "    return train_inds,validation_inds,return_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_the_data(epochs,train_inds,validation_inds,full_epoch_tmin=0,full_epoch_tmax=4,tmin=1,tmax=2):\n",
    "    #returns a dictionary containing the cropped and uncropped versions of the validation and training epochs.\n",
    "    tmin=float(tmin)\n",
    "    tmax=float(tmax)\n",
    "    #save uncropped versions of the data: \n",
    "    #save the training data:\n",
    "    train_set_data_uncroped=epochs.get_data()[train_inds]\n",
    "    train_set_labels_uncroped=epochs.events[train_inds,-1]\n",
    "\n",
    "    #save the validation data: \n",
    "    validation_set_data_uncroped=epochs.get_data()[validation_inds]\n",
    "    validation_Set_labels_uncroped=epochs.events[validation_inds,-1]\n",
    "\n",
    "    #crop the epochs (use the epochs structure)\n",
    "    epochs_cropped = epochs.copy().crop(tmin=full_epoch_tmin, tmax=full_epoch_tmax)\n",
    "\n",
    "    #from here on - we extract the data as matrices (not epoch object anymore):\n",
    "\n",
    "    #save the training data:\n",
    "    train_set_data=epochs_cropped.get_data()[train_inds]\n",
    "    train_set_labels=epochs_cropped.events[train_inds,-1]\n",
    "\n",
    "    #save the validation data: \n",
    "    validation_set_data=epochs_cropped.get_data()[validation_inds]\n",
    "    validation_set_labels=epochs_cropped.events[validation_inds,-1]\n",
    "\n",
    "    return_dict={'train_set_data_uncroped':train_set_data_uncroped,\n",
    "                'train_set_labels_uncroped':train_set_labels_uncroped,\n",
    "                'validation_set_data_uncroped':validation_set_data_uncroped,\n",
    "                'validation_Set_labels_uncroped':validation_Set_labels_uncroped,\n",
    "                'epochs_cropped':epochs_cropped,\n",
    "                'train_set_data':train_set_data,\n",
    "                'train_set_labels':train_set_labels,\n",
    "                'validation_set_data':validation_set_data,\n",
    "                'validation_set_labels':validation_set_labels}\n",
    "    return return_dict\n",
    "\n",
    "def plot_accuracy_over_time(scores_windows,w_times,params_dict,axes_handle):\n",
    "    #this function accepts the scores windows (a list of n folds - each giving a score on a time window)\n",
    "    #it converts it to a long dataframe with the following columns: fold_id,Time,Accuracy\n",
    "    #then it uses the long format to plot using seaborn lineplot and get a confidence interval\n",
    "    times_col_names=[np.round(w_times[s],2) for s in range(len(w_times))]\n",
    "    scores_windows_array=np.squeeze(np.array(scores_windows))\n",
    "    scores_windows_df=pd.DataFrame(columns=times_col_names,data=scores_windows_array)\n",
    "    scores_windows_df['fold_id']=range(len(scores_windows_df))\n",
    "    longform_scores_windows_df=pd.melt(scores_windows_df, id_vars='fold_id', value_vars=scores_windows_df.columns)\n",
    "    longform_scores_windows_df.rename(columns={'variable':'Time','value':'Accuracy'},inplace=True)\n",
    "    sns.lineplot(data=longform_scores_windows_df,x='Time',y='Accuracy',ax=axes_handle)\n",
    "\n",
    "    if any(w_times>0):\n",
    "        onset_location=np.round(w_times[w_times>=0][0],2) ## find the onest (assuming 0 in epoch time)\n",
    "        axes_handle.axvline(onset_location, linestyle='--', color='k', label='Onset') \n",
    "        axes_handle.axvline(onset_location+1, linestyle=':', color='k', label='ArrowGone') ## adjusted 'Onset' to 3 seconds for augmented data\n",
    "    axes_handle.axhline(0.5, linestyle='-', color='k', label='Chance')\n",
    "    axes_handle.set_xlabel('time (s)')\n",
    "    axes_handle.set_ylabel('classification accuracy')\n",
    "    axes_handle.set_title('Classification score over time')\n",
    "    axes_handle.set_ylim([0.25, 0.9])\n",
    "    \n",
    "def run_windowed_classification_on_fold(fold_train_data_x,fold_train_data_y,fold_test_data_x_uncroped,fold_test_data_y,params_dict,w_start,w_length):\n",
    "    #note that this is currently the  function that really does the classification and extracts the performence measure (the previous calls to run_lda.... for example, are just tests)\n",
    "    curr_classifier_name=params_dict['pipeline_name']\n",
    "    if curr_classifier_name=='csp+lda':  \n",
    "        #define the classifier components:  \n",
    "        lda = LinearDiscriminantAnalysis()\n",
    "        csp = CSP(n_components=params_dict['n_components'], reg=None, log=True, norm_trace=False)\n",
    "        #define the pipeline: \n",
    "        clf = Pipeline([('csp',csp),('classifier_LDA',lda)])\n",
    "    elif curr_classifier_name=='ts+lda':\n",
    "        #define the classifier components:  \n",
    "        covest = Covariances()\n",
    "        ts = TangentSpace()\n",
    "        lda = LinearDiscriminantAnalysis()\n",
    "        #define the pipeline: \n",
    "        clf = Pipeline([('conv',covest),('ts', ts), ('LDA', lda)])\n",
    "    elif curr_classifier_name=='fbcsp+lda':\n",
    "        #define the classifier components: \n",
    "        lda = LinearDiscriminantAnalysis()\n",
    "        csp = CSP(n_components=params_dict['n_components_fbcsp'], reg=None, log=True, norm_trace=False)\n",
    "        fb=FilterBank(csp)\n",
    "        #define the pipeline: \n",
    "        clf = Pipeline([('fbcsp',fb),('classifier_LDA',lda)])\n",
    "    else: \n",
    "        raise Exception(f'the requested classifier is not defined in \"run_windowed_classification_on_fold\": {curr_classifier_name}')\n",
    "    \n",
    "    #get string labels instead of numeric (so the classifier will have an informative clf.classes_ )\n",
    "    triggers_label_dict={val:key for key,val in params_dict['preprocessing_dict']['events_triggers_dict'].items()} \n",
    "    fold_train_data_y_labels=np.array([triggers_label_dict[cur_y] for cur_y in fold_train_data_y])  \n",
    "    #fit the selected classifier: \n",
    "    \n",
    "    clf.fit(fold_train_data_x, fold_train_data_y_labels)\n",
    "    # running classifier: test classifier on sliding window\n",
    "\n",
    "    #get string labels instead of numeric for the test\n",
    "    fold_test_data_y_labels=np.array([triggers_label_dict[cur_y] for cur_y in fold_test_data_y])\n",
    "    fold_windowed_scores,confusion_metrices_per_window=run_windowed_pretrained_classifier(clf,fold_test_data_x_uncroped,fold_test_data_y_labels,w_start,w_length)\n",
    "    return fold_windowed_scores,confusion_metrices_per_window,clf\n",
    "\n",
    "def run_windowed_pretrained_classifier(clf,x_uncropped,y,w_start,w_length):\n",
    "    scores_per_time_window = []\n",
    "    confusion_metrices_per_window=[]\n",
    "    if len(x_uncropped.shape)==3: #reshape it as if it was a 4d matrix (assuming the 4th dimention is the filterbank)\n",
    "        x_uncropped=x_uncropped.reshape(list(x_uncropped.shape)+[1])\n",
    "    for n in w_start:\n",
    "        fold_data=np.squeeze(x_uncropped[:, :, n:(n + w_length),:]) #using squeeze here so that if the 4th dimention size is 1 it will reduce it to a 3d vector\n",
    "        #if the classifier uses a filterbank its input should be 4d (trials,channels,timesteps,filter_bands) and if it doesnt its 3d (trials,channels,timesteps)\n",
    "        fold_score_on_time_window=clf.score(fold_data, y)\n",
    "        #append the score for the LDA, using this csp to predict the relevant test scores: \n",
    "        scores_per_time_window.append(fold_score_on_time_window)\n",
    "        confusion_mat=confusion_matrix(y,clf.predict(fold_data),labels=clf.classes_)\n",
    "        confusion_metrices_per_window.append(confusion_mat)\n",
    "    return scores_per_time_window,confusion_metrices_per_window\n",
    "\n",
    "def augment_data(augmentation_params,data_x_to_augment,y,sfreq):\n",
    "    #do augmentation: \n",
    "    if (augmentation_params['win_step']==0 or augmentation_params['win_len']==0): #check if augmentation is not requested/invalid:\n",
    "        augmented_x=data_x_to_augment\n",
    "        augmented_y=y\n",
    "    else: #augmentation requested\n",
    "        #set up the augmentation window boundaries based on the augmentation paramaters:                      \n",
    "        aug_epochs_s=np.arange(0,data_x_to_augment.shape[2],augmentation_params['win_step']*sfreq)\n",
    "        aug_epochs_e=np.array([a+augmentation_params['win_len']*sfreq for a in aug_epochs_s])\n",
    "        #remove start and ends that exceeds the relevant epoch lengths: \n",
    "        aug_epochs_s=aug_epochs_s[aug_epochs_e<data_x_to_augment.shape[2]]\n",
    "        aug_epochs_e=aug_epochs_e[aug_epochs_e<data_x_to_augment.shape[2]]\n",
    "\n",
    "        #pile all augmented (sub windows) to have the regular structure of epochs (>original due to augmentation,channels,samples)\n",
    "        data_fold_x_augmented=[]\n",
    "        data_fold_y_augmented=[]\n",
    "        for aug_s,aug_e in zip(aug_epochs_s,aug_epochs_e):\n",
    "            if len(data_x_to_augment.shape)==3:\n",
    "                data_x_in_cur_window=data_x_to_augment[:,:,int(aug_s):int(aug_e)]\n",
    "            elif len(data_x_to_augment.shape)==4: #with filterbank: \n",
    "                data_x_in_cur_window=data_x_to_augment[:,:,int(aug_s):int(aug_e),:]\n",
    "            data_y_in_cur_window=y\n",
    "            data_fold_x_augmented.append(data_x_in_cur_window)\n",
    "            data_fold_y_augmented.append(data_y_in_cur_window)\n",
    "\n",
    "        augmented_x=np.concatenate(data_fold_x_augmented,axis=0)\n",
    "        augmented_y=np.concatenate(data_fold_y_augmented)\n",
    "    return augmented_x,augmented_y\n",
    "\n",
    "\n",
    "def run_windowed_classification_aug_cv(epochs_cropped,cv_split,train_set_data,train_set_labels,train_set_data_uncroped,params_dict):\n",
    "    augmentation_params=params_dict['augmentation_params']\n",
    "    windowed_prediction_params=params_dict['windowed_prediction_params']\n",
    "    win_len=windowed_prediction_params['win_len']\n",
    "    win_step=windowed_prediction_params['win_step']\n",
    "    \n",
    "    sfreq = epochs_cropped.info['sfreq']\n",
    "    w_length = int(sfreq * win_len)   # running classifier: window length\n",
    "    w_step = int(sfreq * win_step)  # running classifier: window step size\n",
    "    w_start = np.arange(0, train_set_data_uncroped.shape[2] - w_length, w_step)\n",
    "    print('uncroped train set length = ',train_set_data_uncroped.shape[2])\n",
    "\n",
    "    scores_windows = []\n",
    "    folds_confusion_metrices_per_window=[]\n",
    "    #this section first extracts each CV fold, only then it augments it (to avoid data leakage)\n",
    "    for train_idx, test_idx in cv_split:\n",
    "        #seperate the cv fold for labels - train-test:\n",
    "        y_train, y_test = train_set_labels[train_idx], train_set_labels[test_idx] \n",
    "        #seperate the cv fold for features information: \n",
    "        if len(train_set_data.shape)==3:\n",
    "            data_fold_x_train_to_augment = train_set_data[train_idx,:,:]\n",
    "        elif len(train_set_data.shape)==4: #there are filter bank info in the data: \n",
    "            data_fold_x_train_to_augment = train_set_data[train_idx,:,:,:] \n",
    "        #do augmentation: \n",
    "        augmented_x,augmented_y=augment_data(augmentation_params,data_fold_x_train_to_augment,y_train,sfreq)\n",
    "        #run classifier on the data fold\n",
    "        curr_scores_windows,confusion_metrices_per_window,_=run_windowed_classification_on_fold(augmented_x,augmented_y,train_set_data_uncroped[test_idx],y_test,params_dict,w_start,w_length)         \n",
    "        scores_windows.append(curr_scores_windows)\n",
    "        folds_confusion_metrices_per_window.append(confusion_metrices_per_window)\n",
    "    w_times = (w_start + w_length / 2.) / sfreq + params_dict['epoch_tmin']\n",
    "    return scores_windows,folds_confusion_metrices_per_window,w_times\n",
    "\n",
    "\n",
    "def run_windowed_classification_aug(epochs_cropped,train_set_data,train_set_labels,train_set_data_uncroped,test_y,params_dict):\n",
    "    augmentation_params=params_dict['augmentation_params']\n",
    "    windowed_prediction_params=params_dict['windowed_prediction_params']\n",
    "    win_len=windowed_prediction_params['win_len']\n",
    "    win_step=windowed_prediction_params['win_step']\n",
    "    sfreq = epochs_cropped.info['sfreq']\n",
    "    w_length = int(sfreq * win_len)   # running classifier: window length\n",
    "    w_step = int(sfreq * win_step)  # running classifier: window step size\n",
    "    w_start = np.arange(0, train_set_data_uncroped.shape[2] - w_length, w_step)\n",
    "    w_times = (w_start + w_length / 2.) / sfreq + params_dict['epoch_tmin']\n",
    "\n",
    "    augmented_x,augmented_y=augment_data(augmentation_params,train_set_data,train_set_labels,sfreq)\n",
    "    scores_windows,confusion_metrices_per_window,trained_clf=run_windowed_classification_on_fold(augmented_x,augmented_y,train_set_data_uncroped,test_y,params_dict,w_start,w_length)         \n",
    "\n",
    "    return scores_windows,confusion_metrices_per_window,w_times,trained_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training_and_classification_on_selected_params(params_dict,preprocessing_dict,to_plot=False,figure_outputs_path='',fig_name='temp'):\n",
    "    epochs_copy=preprocessing_dict['epochs']\n",
    "    train_inds=preprocessing_dict['train_inds']\n",
    "    validation_inds=preprocessing_dict['validation_inds']['original_trial_ind'].values\n",
    "\n",
    "    #crop the data according to the training window: \n",
    "    returned_dict=crop_the_data(epochs_copy,train_inds,validation_inds,params_dict['classifier_window_s'],params_dict['classifier_window_e']) #two more paramters here are tmin and tmax which are not used apparently. \n",
    "    train_set_data_uncropped=returned_dict['train_set_data_uncroped']\n",
    "    epochs_cropped=returned_dict['epochs_cropped']\n",
    "    train_set_data=returned_dict['train_set_data']\n",
    "    train_set_labels=returned_dict['train_set_labels']\n",
    "\n",
    "    validation_set_labels=returned_dict['validation_set_labels']\n",
    "    validation_set_data_uncropped=returned_dict['validation_set_data_uncroped']\n",
    "    #define cv on the data: \n",
    "    cv = StratifiedShuffleSplit(10, test_size=0.2, random_state=42)\n",
    "    cv_split = cv.split(train_set_data,train_set_labels)\n",
    "\n",
    "    #filter bank related:\n",
    "    if params_dict['pipeline_name']=='fbcsp+lda': \n",
    "        train_set_data_fb=[]\n",
    "        train_set_data_uncropped_fb=[]\n",
    "        validation_set_data_fb=[]\n",
    "        validation_set_data_uncropped_fb=[]\n",
    "        for filtered_data_band_epoch in preprocessing_dict['filter_bank_epochs']:\n",
    "            returned_dict_temp=crop_the_data(filtered_data_band_epoch,train_inds,validation_inds, params_dict['classifier_window_s'],params_dict['classifier_window_e'])\n",
    "            #extract the train set data: \n",
    "            train_set_data_uncroped_temp=returned_dict_temp['train_set_data_uncroped']\n",
    "            train_set_data_temp=returned_dict_temp['train_set_data']\n",
    "            train_set_data_fb.append(train_set_data_temp)\n",
    "            train_set_data_uncropped_fb.append(train_set_data_uncroped_temp)\n",
    "            #extract the validation set data: \n",
    "            validation_set_data_uncroped_temp=returned_dict_temp['validation_set_data_uncroped']\n",
    "            validation_set_data_temp=returned_dict_temp['validation_set_data']\n",
    "            validation_set_data_fb.append(validation_set_data_temp)\n",
    "            validation_set_data_uncropped_fb.append(validation_set_data_uncroped_temp)\n",
    "        #create a 4d matrix of train data:     \n",
    "        train_set_data_4d_array= np.transpose(np.array(train_set_data_fb),(1,2,3,0))\n",
    "        train_set_data_uncropped_4d_array=np.transpose(np.array(train_set_data_uncropped_fb),(1,2,3,0)) \n",
    "        train_set_data=train_set_data_4d_array\n",
    "        train_set_data_uncropped=train_set_data_uncropped_4d_array\n",
    "        #create a 4d matrix of validation data: \n",
    "        validation_set_data_4d_array= np.transpose(np.array(validation_set_data_fb),(1,2,3,0))\n",
    "        validation_set_data_uncropped_4d_array=np.transpose(np.array(validation_set_data_uncropped_fb),(1,2,3,0)) \n",
    "        validation_set_data_uncropped=validation_set_data_uncropped_4d_array\n",
    "\n",
    "    #get scores over time using CV: \n",
    "    scores_windows,folds_confusion_metrices_per_window,w_times=run_windowed_classification_aug_cv(epochs_cropped,cv_split,train_set_data,train_set_labels,train_set_data_uncropped,params_dict)\n",
    "    \n",
    "    #train the classifier based on ALL training data, and test its prediction on the unseen validation set: \n",
    "    validaiton_scores,validation_confusion_metrices_per_window,_,trained_clf=run_windowed_classification_aug(epochs_cropped,train_set_data,train_set_labels,validation_set_data_uncropped,validation_set_labels,params_dict)\n",
    "    if to_plot:\n",
    "        fig,axes=plt.subplots(nrows=1,ncols=2)\n",
    "        plot_accuracy_over_time(scores_windows,w_times,params_dict,axes_handle=axes[0])\n",
    "        epochs_copy.plot_sensors(show_names=True,axes=axes[1])\n",
    "        figname=fig_name + '.svg'\n",
    "        fig.savefig(figure_outputs_path / figname)\n",
    "    else:\n",
    "        fig=[]\n",
    "    return fig,w_times,scores_windows,folds_confusion_metrices_per_window,validaiton_scores,validation_confusion_metrices_per_window,trained_clf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_grid_search_on_single_participant(grid_search_dict,recording_file,Subject,save_every_n_iter,save_location_path,to_plot=True):\n",
    "    all_options=[list(range(len(val))) for key,val in grid_search_dict.items()]\n",
    "    #get all possible grid_search combinations: \n",
    "    all_grid_combinations = list(itertools.product(*all_options))\n",
    "    print(f'number of grid search iterations: {len(all_grid_combinations)}')\n",
    "\n",
    "    grid_search_data_frame_info=pd.DataFrame()\n",
    "    print('running grid search on:',recording_file)\n",
    "    #put all in a single params_dictionary for the current run: \n",
    "    #run all grid_search iterations: \n",
    "    for iteration_ind in tqdm(range(len(all_grid_combinations))):\n",
    "        #extract current iteration paramaters:\n",
    "        params_dict=set_up_params_for_current_grid_iteration(all_grid_combinations,iteration_ind,grid_search_dict)\n",
    "        \n",
    "        #add subject specific information: \n",
    "        params_dict['recording_file']=recording_file\n",
    "        params_dict['subject']=Subject\n",
    "        print(f'test iteration: dictionary paramaters: {params_dict}')\n",
    "        #run preprocessing:\n",
    "        train_inds,validation_inds,preprocessing_dict=run_pre_processing_extract_validation_set(recording_path,current_path,params_dict)\n",
    "        #run prediction on cross validation:\n",
    "        fig,w_times,scores_windows,validation_scores=run_training_and_classification_on_selected_params(params_dict,preprocessing_dict,to_plot=to_plot,figure_outputs_path=figure_outputs_path,fig_name='test')\n",
    "        #train the model on all the training data: \n",
    "        #   tbd\n",
    "        #run prediction on the validation set(?)\n",
    "        #   tbd\n",
    "        #add scores related information: \n",
    "        curr_params_df=pd.DataFrame([params_dict],index=[iteration_ind])\n",
    "        curr_params_df['mean_scores']=np.nan\n",
    "        curr_params_df['std_scores']=np.nan\n",
    "        curr_params_df['mean_scores']=curr_params_df['mean_scores'].astype(object)\n",
    "        curr_params_df['std_scores']=curr_params_df['mean_scores'].astype(object)\n",
    "        curr_params_df['mean_scores']=[np.mean(scores_windows,axis=1)]\n",
    "        curr_params_df['std_scores']=[np.std(scores_windows,axis=1)]\n",
    "        grid_search_data_frame_info=pd.concat([grid_search_data_frame_info,curr_params_df],axis=0)\n",
    "        df_name='hypter_param_search_' + recording_file.split('.')[0] + '.csv'\n",
    "        if np.mod(iteration_ind,save_every_n_iter)==0:\n",
    "            print('saving')\n",
    "            grid_search_data_frame_info.to_csv(hyper_param_search_output / df_name)\n",
    "\n",
    "    #save all grid_search results: \n",
    "    grid_search_data_frame_info.to_csv(hyper_param_search_output / df_name)\n",
    "    return grid_search_data_frame_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_precision_recall_curves_from_trained_classifier(preprocessing_dict,params_dict,precision_recall_curve_timerange,trained_clf,predict_validation=True):\n",
    "    #to learn on precision recall curves see :https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html \n",
    "    #the code is adapated for our usage: \n",
    "    #prepreocessing_dict - dictionary that contains the {original epoched data, and the training/validation indexes}\n",
    "    #params_dict - dictionary that contains the prediction paramaters\n",
    "    #precision_Revall_curve_timerange - list #in seconds relative to epoch (so if epoch is -3 to +4, 3-4 will take the last second in the epoch )\n",
    "    #trained_clf - the classifier that was previously trained on all the data: (note that this means that the report here is biased (better than really is))\n",
    "    #predict_validation - true - will use only validation indexes, false - will use only training indexes (much more biased ofcourse) \n",
    "    \n",
    "    #define what time_range you want to extract the recall/precision for: \n",
    "    print(f'chosen window prediction range is {precision_recall_curve_timerange}\\nnote that the prediction paramaters (that the classifier is trained on) are: {params_dict[\"windowed_prediction_params\"]}\\nconsider if you want the preciction range to match the prediction_param')\n",
    "    \n",
    "    #decide if we use the training or the validation set to plot: \n",
    "    if predict_validation: \n",
    "    #get the relevant data for the validation set: \n",
    "        inds=preprocessing_dict['validation_inds']['original_trial_ind'].values\n",
    "    else:\n",
    "        inds=preprocessing_dict['train_inds']\n",
    "\n",
    "\n",
    "    #extract the labels: \n",
    "    labels=preprocessing_dict['epochs'].events[inds, -1]\n",
    "    #extract the decision function: \n",
    "    decision_function=trained_clf.decision_function(preprocessing_dict['epochs'].copy().crop(tmin=precision_recall_curve_timerange[0],tmax=precision_recall_curve_timerange[1]).get_data()[inds,:])\n",
    "    y_score=decision_function\n",
    "    # Use label_binarize to be multi-label like settings (basicly the current label position is 1 and rest are 0): \n",
    "    #so the label list of say, 0 2 4 4 will output = [1,0,0],[0,1,0],[0,0,1],[0,0,1]\n",
    "    classes_numeric_list=list(preprocessing_dict['events_triggers_dict'].values())\n",
    "    classes_names_list=list(preprocessing_dict['events_triggers_dict'].keys())\n",
    "    #take the classes from the preprocessing dict:\n",
    "    Y = label_binarize(labels, classes=classes_numeric_list)\n",
    "    n_classes = Y.shape[1]\n",
    "\n",
    "    #calculate precision and recall for each class\n",
    "    precision = dict()\n",
    "    recall = dict()\n",
    "    average_precision = dict()\n",
    "    thresholds=dict()\n",
    "    if n_classes==1: #in a binary setting where the score is only relates to being in group \"1\" (or maybe 0, worth checking)\n",
    "        precision[0], recall[0], thresholds[0]  = precision_recall_curve(Y, y_score)\n",
    "        average_precision[0] = average_precision_score(Y, y_score)\n",
    "    else: \n",
    "        for i in range(n_classes):\n",
    "            precision[i], recall[i], thresholds[i] = precision_recall_curve(Y[:, i], y_score[:, i])\n",
    "            average_precision[i] = average_precision_score(Y[:, i], y_score[:, i])\n",
    "\n",
    "    # A \"micro-average\": quantifying score on all classes jointly\n",
    "    precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(Y.ravel(), y_score.ravel())\n",
    "    average_precision[\"micro\"] = average_precision_score(Y, y_score, average=\"micro\")\n",
    "\n",
    "    # setup plot details\n",
    "    colors = cycle([\"navy\", \"turquoise\", \"darkorange\", \"cornflowerblue\", \"teal\"])\n",
    "\n",
    "    _, ax = plt.subplots(figsize=(7, 8))\n",
    "\n",
    "    f_scores = np.linspace(0.2, 0.8, num=4)\n",
    "    lines, labels = [], []\n",
    "    for f_score in f_scores:\n",
    "        x = np.linspace(0.01, 1)\n",
    "        y = f_score * x / (2 * x - f_score)\n",
    "        (l,) = plt.plot(x[y >= 0], y[y >= 0], color=\"gray\", alpha=0.2)\n",
    "        plt.annotate(\"f1={0:0.1f}\".format(f_score), xy=(0.9, y[45] + 0.02))\n",
    "\n",
    "    display = PrecisionRecallDisplay(\n",
    "        recall=recall[\"micro\"],\n",
    "        precision=precision[\"micro\"],\n",
    "        average_precision=average_precision[\"micro\"],\n",
    "    )\n",
    "    display.plot(ax=ax, name=\"Micro-average precision-recall\", color=\"gold\")\n",
    "\n",
    "    for i, color in zip(range(n_classes), colors):\n",
    "        display = PrecisionRecallDisplay(\n",
    "            recall=recall[i],\n",
    "            precision=precision[i],\n",
    "            average_precision=average_precision[i],\n",
    "        )\n",
    "        display.plot(ax=ax, name=f\"Precision-recall for class {classes_names_list[i]}\", color=color)\n",
    "\n",
    "    # add the legend for the iso-f1 curves\n",
    "    handles, labels = display.ax_.get_legend_handles_labels()\n",
    "    handles.extend([l])\n",
    "    labels.extend([\"iso-f1 curves\"])\n",
    "    # set the legend and the axes\n",
    "    ax.set_xlim([0.0, 1.0])\n",
    "    ax.set_ylim([0.0, 1.05])\n",
    "    ax.legend(handles=handles, labels=labels, loc=\"best\")\n",
    "    ax.set_title(f\"multi-class Precision-Recall curve\\npredicted time range: {precision_recall_curve_timerange}\")\n",
    "\n",
    "    plt.show()\n",
    "    #create a dataframe with all information relevant to the plot. \n",
    "    precision.pop('micro')\n",
    "    precision_df=pd.DataFrame(precision)\n",
    "    precision_df.columns=['precision_'+str(colname) for colname in precision_df.columns] \n",
    "    recall.pop('micro')\n",
    "    recall_df=pd.DataFrame(recall)\n",
    "    recall_df.columns=['recall_'+str(colname) for colname in recall_df.columns]\n",
    "    thresholds_df=pd.DataFrame(thresholds)\n",
    "    thresholds_df.columns=['thresholds'+str(colname) for colname in thresholds_df.columns]\n",
    "\n",
    "    return_df=pd.concat([precision_df,recall_df,thresholds_df],axis=1)\n",
    "\n",
    "\n",
    "    return return_df\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## single participant single grid-iteration: \n",
    "### in the next cell we define a specific iteration on a specific participant from the grid and run it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sub-Roei_ses-MI1_task-Default_run-001_eeg.xdf',\n",
       " 'sub-Roei_ses-MI2_task-Default_run-001_eeg.xdf',\n",
       " 'sub-Roei_ses-Mi3_task-Default_run-001_eeg.xdf']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recording_files[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sub-Roei', 'sub-Roei', 'sub-Roei']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subject_names[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NH_Block_1.xdf NH\n",
      "NH_Block_2.xdf NH\n",
      "NH_Block_3.xdf NH\n",
      "setting up current params: iteration 0 - grid settings: (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n",
      "NH_Block_1.xdf NH\n",
      "Creating RawArray with float64 data, n_channels=67, n_times=333262\n",
      "    Range : 0 ... 333261 =      0.000 ...   666.522 secs\n",
      "Ready.\n",
      "\n",
      "###########################################################\n",
      "removing subject sepecific bad electrodes from the raw data\n",
      "\n",
      "###########################################################\n",
      "removing bad channels from epochs:\n",
      "\n",
      "###########################################################\n",
      "running csd\n",
      "Fitted sphere radius:         95.0 mm\n",
      "Origin head coordinates:      0.0 -0.0 0.0 mm\n",
      "Origin device coordinates:    0.0 -0.0 0.0 mm\n",
      "\n",
      "###########################################################\n",
      "filtering the data\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 8 - 32 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 8.00\n",
      "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 7.00 Hz)\n",
      "- Upper passband edge: 32.00 Hz\n",
      "- Upper transition bandwidth: 8.00 Hz (-6 dB cutoff frequency: 36.00 Hz)\n",
      "- Filter length: 825 samples (1.650 sec)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.0s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 8 - 12 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 8.00\n",
      "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 7.00 Hz)\n",
      "- Upper passband edge: 12.00 Hz\n",
      "- Upper transition bandwidth: 3.00 Hz (-6 dB cutoff frequency: 13.50 Hz)\n",
      "- Filter length: 825 samples (1.650 sec)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  63 out of  63 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.0s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 12 - 20 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 12.00\n",
      "- Lower transition bandwidth: 3.00 Hz (-6 dB cutoff frequency: 10.50 Hz)\n",
      "- Upper passband edge: 20.00 Hz\n",
      "- Upper transition bandwidth: 5.00 Hz (-6 dB cutoff frequency: 22.50 Hz)\n",
      "- Filter length: 551 samples (1.102 sec)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  63 out of  63 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.0s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Annotations descriptions: ['Beep', 'Experiment Ended', 'Left', 'Long Break', 'Rest', 'Resting', 'Right']\n",
      "\n",
      "###########################################################\n",
      "extracting event info: {'Beep': 1, 'Experiment Ended': 2, 'Left': 3, 'Long Break': 4, 'Rest': 5, 'Resting': 6, 'Right': 7}\n",
      "\n",
      "###########################################################\n",
      "Not setting metadata\n",
      "60 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 60 events and 3501 original time points ...\n",
      "0 bad epochs dropped\n",
      "Not setting metadata\n",
      "60 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 60 events and 3501 original time points ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  63 out of  63 | elapsed:    1.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 bad epochs dropped\n",
      "epoching + selecting current electodes set for analysis:\n",
      "['C5', 'C3', 'C1', 'Cz', 'C2', 'C4', 'C6']\n",
      "Not setting metadata\n",
      "60 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 60 events and 3501 original time points ...\n",
      "0 bad epochs dropped\n",
      "\n",
      "###########################################################\n",
      "removing bad channels from epochs:\n",
      "\n",
      "###########################################################\n",
      "the current selected electrodes: {'C3', 'C2', 'C4', 'C6', 'C5', 'Cz', 'C1'} allready exclude the requested electrodes to remove {'TP7'}\n",
      "#############################################################\n",
      "putting aside 20% of the data: trial numbers are:\n",
      " [43 53  5 39  7 19 17 34  2 51 14 20]\n",
      "\n",
      "remaining 80% of the trials go into training for cv:\n",
      " [11 46 30 58 41 48 16  8 42  4 23 32 35 28  0 47 22 56 40 37 13 33 31 21\n",
      " 57 18 45 24  9 10 36 54 29 50 15 27 49 44 38 12 52  1 26  6 55 59  3 25]\n",
      "\n",
      "NH_Block_2.xdf NH\n",
      "Creating RawArray with float64 data, n_channels=67, n_times=346498\n",
      "    Range : 0 ... 346497 =      0.000 ...   692.994 secs\n",
      "Ready.\n",
      "\n",
      "###########################################################\n",
      "removing subject sepecific bad electrodes from the raw data\n",
      "\n",
      "###########################################################\n",
      "removing bad channels from epochs:\n",
      "\n",
      "###########################################################\n",
      "running csd\n",
      "Fitted sphere radius:         95.0 mm\n",
      "Origin head coordinates:      0.0 -0.0 0.0 mm\n",
      "Origin device coordinates:    0.0 -0.0 0.0 mm\n",
      "\n",
      "###########################################################\n",
      "filtering the data\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 8 - 32 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 8.00\n",
      "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 7.00 Hz)\n",
      "- Upper passband edge: 32.00 Hz\n",
      "- Upper transition bandwidth: 8.00 Hz (-6 dB cutoff frequency: 36.00 Hz)\n",
      "- Filter length: 825 samples (1.650 sec)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.0s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 8 - 12 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 8.00\n",
      "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 7.00 Hz)\n",
      "- Upper passband edge: 12.00 Hz\n",
      "- Upper transition bandwidth: 3.00 Hz (-6 dB cutoff frequency: 13.50 Hz)\n",
      "- Filter length: 825 samples (1.650 sec)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  63 out of  63 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.0s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 12 - 20 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 12.00\n",
      "- Lower transition bandwidth: 3.00 Hz (-6 dB cutoff frequency: 10.50 Hz)\n",
      "- Upper passband edge: 20.00 Hz\n",
      "- Upper transition bandwidth: 5.00 Hz (-6 dB cutoff frequency: 22.50 Hz)\n",
      "- Filter length: 551 samples (1.102 sec)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  63 out of  63 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.0s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Annotations descriptions: ['Beep', 'Experiment Ended', 'Left', 'Long Break', 'Rest', 'Resting', 'Right']\n",
      "\n",
      "###########################################################\n",
      "extracting event info: {'Beep': 1, 'Experiment Ended': 2, 'Left': 3, 'Long Break': 4, 'Rest': 5, 'Resting': 6, 'Right': 7}\n",
      "\n",
      "###########################################################\n",
      "Not setting metadata\n",
      "60 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 60 events and 3501 original time points ...\n",
      "0 bad epochs dropped\n",
      "Not setting metadata\n",
      "60 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 60 events and 3501 original time points ...\n",
      "0 bad epochs dropped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  63 out of  63 | elapsed:    1.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoching + selecting current electodes set for analysis:\n",
      "['C5', 'C3', 'C1', 'Cz', 'C2', 'C4', 'C6']\n",
      "Not setting metadata\n",
      "60 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 60 events and 3501 original time points ...\n",
      "0 bad epochs dropped\n",
      "\n",
      "###########################################################\n",
      "removing bad channels from epochs:\n",
      "\n",
      "###########################################################\n",
      "the current selected electrodes: {'C3', 'C2', 'C4', 'C6', 'C5', 'Cz', 'C1'} allready exclude the requested electrodes to remove {'TP7'}\n",
      "#############################################################\n",
      "putting aside 20% of the data: trial numbers are:\n",
      " [36 42 11 45 10 18 16 27  1 53 14 19]\n",
      "\n",
      "remaining 80% of the trials go into training for cv:\n",
      " [12 40 22 58 38 41 17  4 32  8 20 43 28 37  0 50 29 56 46 44 13 25 39 24\n",
      " 47 21 49 23  5  6 31 55 33 52 15 35 51 48 34  7 54  3 30  9 57 59  2 26]\n",
      "\n",
      "NH_Block_3.xdf NH\n",
      "Creating RawArray with float64 data, n_channels=67, n_times=296047\n",
      "    Range : 0 ... 296046 =      0.000 ...   592.092 secs\n",
      "Ready.\n",
      "\n",
      "###########################################################\n",
      "removing subject sepecific bad electrodes from the raw data\n",
      "\n",
      "###########################################################\n",
      "removing bad channels from epochs:\n",
      "\n",
      "###########################################################\n",
      "running csd\n",
      "Fitted sphere radius:         95.0 mm\n",
      "Origin head coordinates:      0.0 -0.0 0.0 mm\n",
      "Origin device coordinates:    0.0 -0.0 0.0 mm\n",
      "\n",
      "###########################################################\n",
      "filtering the data\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 8 - 32 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 8.00\n",
      "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 7.00 Hz)\n",
      "- Upper passband edge: 32.00 Hz\n",
      "- Upper transition bandwidth: 8.00 Hz (-6 dB cutoff frequency: 36.00 Hz)\n",
      "- Filter length: 825 samples (1.650 sec)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.0s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 8 - 12 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 8.00\n",
      "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 7.00 Hz)\n",
      "- Upper passband edge: 12.00 Hz\n",
      "- Upper transition bandwidth: 3.00 Hz (-6 dB cutoff frequency: 13.50 Hz)\n",
      "- Filter length: 825 samples (1.650 sec)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  63 out of  63 | elapsed:    0.8s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.0s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 12 - 20 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 12.00\n",
      "- Lower transition bandwidth: 3.00 Hz (-6 dB cutoff frequency: 10.50 Hz)\n",
      "- Upper passband edge: 20.00 Hz\n",
      "- Upper transition bandwidth: 5.00 Hz (-6 dB cutoff frequency: 22.50 Hz)\n",
      "- Filter length: 551 samples (1.102 sec)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  63 out of  63 | elapsed:    0.8s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.0s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Annotations descriptions: ['Beep', 'Experiment Ended', 'Left', 'Long Break', 'Rest', 'Resting', 'Right']\n",
      "\n",
      "###########################################################\n",
      "extracting event info: {'Beep': 1, 'Experiment Ended': 2, 'Left': 3, 'Long Break': 4, 'Rest': 5, 'Resting': 6, 'Right': 7}\n",
      "\n",
      "###########################################################\n",
      "Not setting metadata\n",
      "60 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 60 events and 3501 original time points ...\n",
      "0 bad epochs dropped\n",
      "Not setting metadata\n",
      "60 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 60 events and 3501 original time points ...\n",
      "0 bad epochs dropped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  63 out of  63 | elapsed:    1.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoching + selecting current electodes set for analysis:\n",
      "['C5', 'C3', 'C1', 'Cz', 'C2', 'C4', 'C6']\n",
      "Not setting metadata\n",
      "60 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 60 events and 3501 original time points ...\n",
      "0 bad epochs dropped\n",
      "\n",
      "###########################################################\n",
      "removing bad channels from epochs:\n",
      "\n",
      "###########################################################\n",
      "the current selected electrodes: {'C3', 'C2', 'C4', 'C6', 'C5', 'Cz', 'C1'} allready exclude the requested electrodes to remove {'TP7'}\n",
      "#############################################################\n",
      "putting aside 20% of the data: trial numbers are:\n",
      " [47 49  3 37  8 17 24 31  1 52 20 21]\n",
      "\n",
      "remaining 80% of the trials go into training for cv:\n",
      " [10 45 22 59 44 50  7  9 41  4 26 28 34 25  0 46 18 56 39 32 12 29 27 15\n",
      " 58 11 43 30 13 14 40 53 36 51 23 19 48 42 38 16 54  5 35  6 55 57  2 33]\n",
      "\n",
      "Not setting metadata\n",
      "120 matching events found\n",
      "No baseline correction applied\n",
      "Not setting metadata\n",
      "120 matching events found\n",
      "No baseline correction applied\n",
      "Not setting metadata\n",
      "120 matching events found\n",
      "No baseline correction applied\n",
      "Not setting metadata\n",
      "180 matching events found\n",
      "No baseline correction applied\n",
      "Not setting metadata\n",
      "180 matching events found\n",
      "No baseline correction applied\n",
      "Not setting metadata\n",
      "180 matching events found\n",
      "No baseline correction applied\n",
      "uncroped train set length =  3501\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.011 (2.2e-16 eps * 7 dim * 6.8e+12  max singular value)\n",
      "    Estimated rank (mag): 7\n",
      "    MAG: rank 7 computed from 7 data channels with 0 projectors\n",
      "Reducing data rank from 7 -> 7\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.01 (2.2e-16 eps * 7 dim * 6.6e+12  max singular value)\n",
      "    Estimated rank (mag): 7\n",
      "    MAG: rank 7 computed from 7 data channels with 0 projectors\n",
      "Reducing data rank from 7 -> 7\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.01 (2.2e-16 eps * 7 dim * 6.6e+12  max singular value)\n",
      "    Estimated rank (mag): 7\n",
      "    MAG: rank 7 computed from 7 data channels with 0 projectors\n",
      "Reducing data rank from 7 -> 7\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.01 (2.2e-16 eps * 7 dim * 6.7e+12  max singular value)\n",
      "    Estimated rank (mag): 7\n",
      "    MAG: rank 7 computed from 7 data channels with 0 projectors\n",
      "Reducing data rank from 7 -> 7\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.01 (2.2e-16 eps * 7 dim * 6.6e+12  max singular value)\n",
      "    Estimated rank (mag): 7\n",
      "    MAG: rank 7 computed from 7 data channels with 0 projectors\n",
      "Reducing data rank from 7 -> 7\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.01 (2.2e-16 eps * 7 dim * 6.7e+12  max singular value)\n",
      "    Estimated rank (mag): 7\n",
      "    MAG: rank 7 computed from 7 data channels with 0 projectors\n",
      "Reducing data rank from 7 -> 7\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.01 (2.2e-16 eps * 7 dim * 6.6e+12  max singular value)\n",
      "    Estimated rank (mag): 7\n",
      "    MAG: rank 7 computed from 7 data channels with 0 projectors\n",
      "Reducing data rank from 7 -> 7\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.01 (2.2e-16 eps * 7 dim * 6.7e+12  max singular value)\n",
      "    Estimated rank (mag): 7\n",
      "    MAG: rank 7 computed from 7 data channels with 0 projectors\n",
      "Reducing data rank from 7 -> 7\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.01 (2.2e-16 eps * 7 dim * 6.5e+12  max singular value)\n",
      "    Estimated rank (mag): 7\n",
      "    MAG: rank 7 computed from 7 data channels with 0 projectors\n",
      "Reducing data rank from 7 -> 7\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.01 (2.2e-16 eps * 7 dim * 6.6e+12  max singular value)\n",
      "    Estimated rank (mag): 7\n",
      "    MAG: rank 7 computed from 7 data channels with 0 projectors\n",
      "Reducing data rank from 7 -> 7\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.01 (2.2e-16 eps * 7 dim * 6.6e+12  max singular value)\n",
      "    Estimated rank (mag): 7\n",
      "    MAG: rank 7 computed from 7 data channels with 0 projectors\n",
      "Reducing data rank from 7 -> 7\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.01 (2.2e-16 eps * 7 dim * 6.6e+12  max singular value)\n",
      "    Estimated rank (mag): 7\n",
      "    MAG: rank 7 computed from 7 data channels with 0 projectors\n",
      "Reducing data rank from 7 -> 7\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.01 (2.2e-16 eps * 7 dim * 6.7e+12  max singular value)\n",
      "    Estimated rank (mag): 7\n",
      "    MAG: rank 7 computed from 7 data channels with 0 projectors\n",
      "Reducing data rank from 7 -> 7\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.01 (2.2e-16 eps * 7 dim * 6.6e+12  max singular value)\n",
      "    Estimated rank (mag): 7\n",
      "    MAG: rank 7 computed from 7 data channels with 0 projectors\n",
      "Reducing data rank from 7 -> 7\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.01 (2.2e-16 eps * 7 dim * 6.7e+12  max singular value)\n",
      "    Estimated rank (mag): 7\n",
      "    MAG: rank 7 computed from 7 data channels with 0 projectors\n",
      "Reducing data rank from 7 -> 7\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.011 (2.2e-16 eps * 7 dim * 6.8e+12  max singular value)\n",
      "    Estimated rank (mag): 7\n",
      "    MAG: rank 7 computed from 7 data channels with 0 projectors\n",
      "Reducing data rank from 7 -> 7\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.01 (2.2e-16 eps * 7 dim * 6.6e+12  max singular value)\n",
      "    Estimated rank (mag): 7\n",
      "    MAG: rank 7 computed from 7 data channels with 0 projectors\n",
      "Reducing data rank from 7 -> 7\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.01 (2.2e-16 eps * 7 dim * 6.5e+12  max singular value)\n",
      "    Estimated rank (mag): 7\n",
      "    MAG: rank 7 computed from 7 data channels with 0 projectors\n",
      "Reducing data rank from 7 -> 7\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.01 (2.2e-16 eps * 7 dim * 6.6e+12  max singular value)\n",
      "    Estimated rank (mag): 7\n",
      "    MAG: rank 7 computed from 7 data channels with 0 projectors\n",
      "Reducing data rank from 7 -> 7\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.01 (2.2e-16 eps * 7 dim * 6.8e+12  max singular value)\n",
      "    Estimated rank (mag): 7\n",
      "    MAG: rank 7 computed from 7 data channels with 0 projectors\n",
      "Reducing data rank from 7 -> 7\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.01 (2.2e-16 eps * 7 dim * 6.6e+12  max singular value)\n",
      "    Estimated rank (mag): 7\n",
      "    MAG: rank 7 computed from 7 data channels with 0 projectors\n",
      "Reducing data rank from 7 -> 7\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.011 (2.2e-16 eps * 7 dim * 6.8e+12  max singular value)\n",
      "    Estimated rank (mag): 7\n",
      "    MAG: rank 7 computed from 7 data channels with 0 projectors\n",
      "Reducing data rank from 7 -> 7\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.01 (2.2e-16 eps * 7 dim * 6.8e+12  max singular value)\n",
      "    Estimated rank (mag): 7\n",
      "    MAG: rank 7 computed from 7 data channels with 0 projectors\n",
      "Reducing data rank from 7 -> 7\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.01 (2.2e-16 eps * 7 dim * 6.5e+12  max singular value)\n",
      "    Estimated rank (mag): 7\n",
      "    MAG: rank 7 computed from 7 data channels with 0 projectors\n",
      "Reducing data rank from 7 -> 7\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.01 (2.2e-16 eps * 7 dim * 6.5e+12  max singular value)\n",
      "    Estimated rank (mag): 7\n",
      "    MAG: rank 7 computed from 7 data channels with 0 projectors\n",
      "Reducing data rank from 7 -> 7\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.011 (2.2e-16 eps * 7 dim * 6.8e+12  max singular value)\n",
      "    Estimated rank (mag): 7\n",
      "    MAG: rank 7 computed from 7 data channels with 0 projectors\n",
      "Reducing data rank from 7 -> 7\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.01 (2.2e-16 eps * 7 dim * 6.5e+12  max singular value)\n",
      "    Estimated rank (mag): 7\n",
      "    MAG: rank 7 computed from 7 data channels with 0 projectors\n",
      "Reducing data rank from 7 -> 7\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.011 (2.2e-16 eps * 7 dim * 6.8e+12  max singular value)\n",
      "    Estimated rank (mag): 7\n",
      "    MAG: rank 7 computed from 7 data channels with 0 projectors\n",
      "Reducing data rank from 7 -> 7\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.01 (2.2e-16 eps * 7 dim * 6.6e+12  max singular value)\n",
      "    Estimated rank (mag): 7\n",
      "    MAG: rank 7 computed from 7 data channels with 0 projectors\n",
      "Reducing data rank from 7 -> 7\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.01 (2.2e-16 eps * 7 dim * 6.6e+12  max singular value)\n",
      "    Estimated rank (mag): 7\n",
      "    MAG: rank 7 computed from 7 data channels with 0 projectors\n",
      "Reducing data rank from 7 -> 7\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.012 (2.2e-16 eps * 7 dim * 7.5e+12  max singular value)\n",
      "    Estimated rank (mag): 7\n",
      "    MAG: rank 7 computed from 7 data channels with 0 projectors\n",
      "Reducing data rank from 7 -> 7\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.012 (2.2e-16 eps * 7 dim * 7.5e+12  max singular value)\n",
      "    Estimated rank (mag): 7\n",
      "    MAG: rank 7 computed from 7 data channels with 0 projectors\n",
      "Reducing data rank from 7 -> 7\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.011 (2.2e-16 eps * 7 dim * 7.4e+12  max singular value)\n",
      "    Estimated rank (mag): 7\n",
      "    MAG: rank 7 computed from 7 data channels with 0 projectors\n",
      "Reducing data rank from 7 -> 7\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "#this cell, loads roi 3 datafiles (assuming they are last in the recording_files list (if not, change it... maybe explicitly name them))\n",
    "#it then, run the same preprocessing pipelines in his files, and concatinate it into a single \"combined_preprocessing_dict\", from here, everything works with the same functions that run on single participants\n",
    "\n",
    "\n",
    "\n",
    "#define which subject to currently check: \n",
    "for recording_file,Subject in zip(recording_files[-3:9],subject_names[-3:]):\n",
    "    print(recording_file,Subject)\n",
    "\n",
    "#this code is custom to aggregate all 3 of roi recordings into a single processing_dict structure. \n",
    "\n",
    "#####################################################\n",
    "#define manually paramaters that we wish to change: \n",
    "#get all possible grid_search combinations: \n",
    "iteration_ind=0 #select some grid search combination - you can manualy change the params after getting the \"params_dict\" below\n",
    "grid_search_dict_copy=grid_search_dict.copy()\n",
    "all_grid_combinations = list(itertools.product(*all_options))\n",
    "#here i can change manually the current iteration params: \n",
    "grid_search_dict_copy['Electorde_Groups_names_grid']=['C']\n",
    "grid_search_dict_copy['filters_bands']=[[[8,12],[12,20]]]#[[[8,12], [12, 16],[16,20],[20,24],[24,28],[28,32]]]\n",
    "#this cell allow to test specific iterations\n",
    "params_dict=set_up_params_for_current_grid_iteration(all_grid_combinations,iteration_ind,grid_search_dict_copy)\n",
    "params_dict['subject']=Subject\n",
    "params_dict['recording_file']=recording_file\n",
    "params_dict['filter_method']='fir'\n",
    "params_dict['LowPass']=8\n",
    "params_dict['HighPass']=32\n",
    "params_dict['augmentation_params']={'win_len': 1, 'win_step': 0.1}\n",
    "params_dict['classifier_window_s']=0.5\n",
    "params_dict['classifier_window_e']=4.5\n",
    "params_dict['windowed_prediction_params']={'win_len': 0.5, 'win_step': 0.1}\n",
    "params_dict['pipeline_name']='csp+lda'\n",
    "params_dict['n_components_fbcsp']=4\n",
    "\n",
    "##########################preprocess each of the recording seperately#########################\n",
    "preprocessing_dicts=[]\n",
    "#get all 3 of roi files: \n",
    "for recording_file,subject in zip(recording_files[-6:-9],subject_names[-6:-9]):\n",
    "    print(recording_file,subject)\n",
    "    params_dict['recording_file']=recording_file\n",
    "    train_inds,validation_inds,preprocessing_dict=run_pre_processing_extract_validation_set(recording_path,current_path,params_dict)\n",
    "    preprocessing_dicts.append(preprocessing_dict)\n",
    "##############################################################################################\n",
    "\n",
    "#combine all preprocessing structures: \n",
    "#combine each file preprocessing_dict into a single dictionary:\n",
    "#take the first roi file and modify its triggers to be consistent with the later two: \n",
    "\n",
    "combined_preprocessing_dict=copy.deepcopy(preprocessing_dicts[0]) #the deep copy is important here as we have a dictionary that contains lists/dictionaries\n",
    "\n",
    "#change the epochs structures triggers information to be consistent with the next 2 files: (roi first file had triggers of 2,5 and 6) \n",
    "combined_preprocessing_dict['epochs'].event_id={'left': 3, 'rest': 6, 'right': 7} #change the event ids to be the same as his other 2 files\n",
    "#combined_preprocessing_dict['epochs'].events[:,2]=combined_preprocessing_dict['epochs'].events[:,2]+1 #change the event numeric data within the epoch structure\n",
    "for fb_num in range(len(combined_preprocessing_dict['filter_bank_epochs'])): #do the same on the filter bank epochs (should be able to handle arbitraty number of bands)\n",
    "    #ombined_preprocessing_dict['filter_bank_epochs'][fb_num].events[:,2]=combined_preprocessing_dict['filter_bank_epochs'][fb_num].events[:,2]+1\n",
    "    combined_preprocessing_dict['filter_bank_epochs'][fb_num].event_id={'left': 3, 'rest': 6, 'right': 7}\n",
    "\n",
    "#now that the first recording file is set as standard - read the other two files and add thier information to the combined dictionary: \n",
    "add_to_inds=combined_preprocessing_dict['epochs'].events.shape[0] \n",
    "for i,cur_preprocessing_dict in enumerate(preprocessing_dicts[1:]): \n",
    "    #aggregate all training_indexes: \n",
    "    combined_preprocessing_dict['train_inds'] = np.concatenate([combined_preprocessing_dict['train_inds'],cur_preprocessing_dict['train_inds']+add_to_inds])\n",
    "    #aggregate all validation indexes\n",
    "    validation_inds_df=cur_preprocessing_dict['validation_inds']\n",
    "    validation_inds_df.index=validation_inds_df.index+add_to_inds\n",
    "    validation_inds_df['original_trial_ind']=validation_inds_df['original_trial_ind']+add_to_inds\n",
    "    combined_preprocessing_dict['validation_inds'] = pd.concat([combined_preprocessing_dict['validation_inds'],validation_inds_df],axis=0)\n",
    "    #increase the number to add to keep indexes consistent (i.e. first was 0-60, next file should have them at the range of 60-120 and so on)\n",
    "    add_to_inds+=cur_preprocessing_dict['epochs'].events.shape[0] #this should allow for files with diffeter number of epochs. \n",
    "\n",
    "    for fb_filter_num in range(len(combined_preprocessing_dict['filter_bank_epochs'])):\n",
    "        combined_preprocessing_dict['filter_bank_epochs'][fb_filter_num]=mne.concatenate_epochs([combined_preprocessing_dict['filter_bank_epochs'][fb_filter_num],cur_preprocessing_dict['filter_bank_epochs'][fb_filter_num]], on_mismatch='warn' , verbose=None)\n",
    "    combined_preprocessing_dict['epochs']=mne.concatenate_epochs([combined_preprocessing_dict['epochs'],cur_preprocessing_dict['epochs']], on_mismatch='warn' , verbose=None)\n",
    "    \n",
    "combined_preprocessing_dict['events_triggers_dict']={'left': 3, 'rest': 6, 'right': 7}\n",
    "params_dict['preprocessing_dict']=preprocessing_dict\n",
    "\n",
    "\n",
    "#test it: \n",
    "fig,w_times,scores_windows,folds_confusion_metrices_per_window,validation_scores,validation_confusion_metrices_per_window,trained_clf=run_training_and_classification_on_selected_params(params_dict,combined_preprocessing_dict,to_plot=True,figure_outputs_path=figure_outputs_path,fig_name='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chosen window prediction range is [0, 5]\n",
      "note that the prediction paramaters (that the classifier is trained on) are: {'win_len': 0.5, 'win_step': 0.1}\n",
      "consider if you want the preciction range to match the prediction_param\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision_0</th>\n",
       "      <th>precision_1</th>\n",
       "      <th>precision_2</th>\n",
       "      <th>recall_0</th>\n",
       "      <th>recall_1</th>\n",
       "      <th>recall_2</th>\n",
       "      <th>thresholds0</th>\n",
       "      <th>thresholds1</th>\n",
       "      <th>thresholds2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-2.434687</td>\n",
       "      <td>-3.107879</td>\n",
       "      <td>-2.838632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.342857</td>\n",
       "      <td>0.342857</td>\n",
       "      <td>0.342857</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-2.055234</td>\n",
       "      <td>-2.626702</td>\n",
       "      <td>-2.655497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-2.002550</td>\n",
       "      <td>-2.584349</td>\n",
       "      <td>-2.447666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.957736</td>\n",
       "      <td>-2.519087</td>\n",
       "      <td>-1.967602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.917306</td>\n",
       "      <td>-2.406640</td>\n",
       "      <td>-1.864639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.387097</td>\n",
       "      <td>0.387097</td>\n",
       "      <td>0.387097</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.907880</td>\n",
       "      <td>-2.137278</td>\n",
       "      <td>-1.863686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.894004</td>\n",
       "      <td>-2.095402</td>\n",
       "      <td>-1.833681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.413793</td>\n",
       "      <td>0.413793</td>\n",
       "      <td>0.413793</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.853300</td>\n",
       "      <td>-2.086692</td>\n",
       "      <td>-1.823061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.825720</td>\n",
       "      <td>-2.068655</td>\n",
       "      <td>-1.815955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.817545</td>\n",
       "      <td>-2.053222</td>\n",
       "      <td>-1.766210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.694988</td>\n",
       "      <td>-2.028528</td>\n",
       "      <td>-1.713025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.480000</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.685161</td>\n",
       "      <td>-2.005938</td>\n",
       "      <td>-1.695037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.652502</td>\n",
       "      <td>-1.921902</td>\n",
       "      <td>-1.655382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.478261</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.640433</td>\n",
       "      <td>-1.873250</td>\n",
       "      <td>-1.629060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.619125</td>\n",
       "      <td>-1.855101</td>\n",
       "      <td>-1.594333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.523810</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.483193</td>\n",
       "      <td>-1.792904</td>\n",
       "      <td>-1.562530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.428930</td>\n",
       "      <td>-1.709072</td>\n",
       "      <td>-1.521627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>-1.386312</td>\n",
       "      <td>-1.702352</td>\n",
       "      <td>-1.447263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>-1.336443</td>\n",
       "      <td>-1.635532</td>\n",
       "      <td>-1.441268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>-1.286276</td>\n",
       "      <td>-1.569757</td>\n",
       "      <td>-1.396045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>-1.272592</td>\n",
       "      <td>-1.481153</td>\n",
       "      <td>-1.244956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>-1.270108</td>\n",
       "      <td>-1.372179</td>\n",
       "      <td>-1.227950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>-1.192435</td>\n",
       "      <td>-1.365421</td>\n",
       "      <td>-1.189918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>-1.153370</td>\n",
       "      <td>-1.358025</td>\n",
       "      <td>-1.177794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>-1.148438</td>\n",
       "      <td>-1.242266</td>\n",
       "      <td>-1.081350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>-1.141073</td>\n",
       "      <td>-1.167087</td>\n",
       "      <td>-1.007177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.900000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>-1.039473</td>\n",
       "      <td>-0.926482</td>\n",
       "      <td>-0.971180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>-1.030220</td>\n",
       "      <td>-0.853307</td>\n",
       "      <td>-0.945163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>-0.977623</td>\n",
       "      <td>-0.713733</td>\n",
       "      <td>-0.840024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.976252</td>\n",
       "      <td>-0.575773</td>\n",
       "      <td>-0.753463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>-0.933003</td>\n",
       "      <td>-0.542419</td>\n",
       "      <td>-0.680092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>-0.790624</td>\n",
       "      <td>-0.391410</td>\n",
       "      <td>-0.674285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>-0.763437</td>\n",
       "      <td>-0.250525</td>\n",
       "      <td>-0.669036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>-0.652049</td>\n",
       "      <td>-0.173192</td>\n",
       "      <td>-0.540573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>-0.531808</td>\n",
       "      <td>0.067298</td>\n",
       "      <td>-0.417830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>-0.480700</td>\n",
       "      <td>0.482578</td>\n",
       "      <td>-0.012111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    precision_0  precision_1  precision_2  recall_0  recall_1  recall_2  \\\n",
       "0      0.333333     0.333333     0.333333  1.000000  1.000000  1.000000   \n",
       "1      0.342857     0.342857     0.342857  1.000000  1.000000  1.000000   \n",
       "2      0.352941     0.352941     0.352941  1.000000  1.000000  1.000000   \n",
       "3      0.363636     0.363636     0.363636  1.000000  1.000000  1.000000   \n",
       "4      0.375000     0.375000     0.375000  1.000000  1.000000  1.000000   \n",
       "5      0.387097     0.387097     0.387097  1.000000  1.000000  1.000000   \n",
       "6      0.400000     0.400000     0.400000  1.000000  1.000000  1.000000   \n",
       "7      0.413793     0.413793     0.413793  1.000000  1.000000  1.000000   \n",
       "8      0.428571     0.428571     0.428571  1.000000  1.000000  1.000000   \n",
       "9      0.444444     0.444444     0.444444  1.000000  1.000000  1.000000   \n",
       "10     0.461538     0.461538     0.461538  1.000000  1.000000  1.000000   \n",
       "11     0.480000     0.480000     0.480000  1.000000  1.000000  1.000000   \n",
       "12     0.500000     0.458333     0.500000  1.000000  0.916667  1.000000   \n",
       "13     0.521739     0.478261     0.521739  1.000000  0.916667  1.000000   \n",
       "14     0.545455     0.500000     0.545455  1.000000  0.916667  1.000000   \n",
       "15     0.571429     0.523810     0.571429  1.000000  0.916667  1.000000   \n",
       "16     0.600000     0.550000     0.600000  1.000000  0.916667  1.000000   \n",
       "17     0.578947     0.578947     0.578947  0.916667  0.916667  0.916667   \n",
       "18     0.611111     0.611111     0.611111  0.916667  0.916667  0.916667   \n",
       "19     0.647059     0.647059     0.647059  0.916667  0.916667  0.916667   \n",
       "20     0.625000     0.687500     0.687500  0.833333  0.916667  0.916667   \n",
       "21     0.666667     0.733333     0.733333  0.833333  0.916667  0.916667   \n",
       "22     0.714286     0.785714     0.785714  0.833333  0.916667  0.916667   \n",
       "23     0.769231     0.846154     0.846154  0.833333  0.916667  0.916667   \n",
       "24     0.833333     0.916667     0.833333  0.833333  0.916667  0.833333   \n",
       "25     0.909091     1.000000     0.818182  0.833333  0.916667  0.750000   \n",
       "26     0.900000     1.000000     0.800000  0.750000  0.833333  0.666667   \n",
       "27     1.000000     1.000000     0.888889  0.750000  0.750000  0.666667   \n",
       "28     1.000000     1.000000     0.875000  0.666667  0.666667  0.583333   \n",
       "29     1.000000     1.000000     0.857143  0.583333  0.583333  0.500000   \n",
       "30     1.000000     1.000000     0.833333  0.500000  0.500000  0.416667   \n",
       "31     1.000000     1.000000     0.800000  0.416667  0.416667  0.333333   \n",
       "32     1.000000     1.000000     0.750000  0.333333  0.333333  0.250000   \n",
       "33     1.000000     1.000000     1.000000  0.250000  0.250000  0.250000   \n",
       "34     1.000000     1.000000     1.000000  0.166667  0.166667  0.166667   \n",
       "35     1.000000     1.000000     1.000000  0.083333  0.083333  0.083333   \n",
       "36     1.000000     1.000000     1.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "    thresholds0  thresholds1  thresholds2  \n",
       "0     -2.434687    -3.107879    -2.838632  \n",
       "1     -2.055234    -2.626702    -2.655497  \n",
       "2     -2.002550    -2.584349    -2.447666  \n",
       "3     -1.957736    -2.519087    -1.967602  \n",
       "4     -1.917306    -2.406640    -1.864639  \n",
       "5     -1.907880    -2.137278    -1.863686  \n",
       "6     -1.894004    -2.095402    -1.833681  \n",
       "7     -1.853300    -2.086692    -1.823061  \n",
       "8     -1.825720    -2.068655    -1.815955  \n",
       "9     -1.817545    -2.053222    -1.766210  \n",
       "10    -1.694988    -2.028528    -1.713025  \n",
       "11    -1.685161    -2.005938    -1.695037  \n",
       "12    -1.652502    -1.921902    -1.655382  \n",
       "13    -1.640433    -1.873250    -1.629060  \n",
       "14    -1.619125    -1.855101    -1.594333  \n",
       "15    -1.483193    -1.792904    -1.562530  \n",
       "16    -1.428930    -1.709072    -1.521627  \n",
       "17    -1.386312    -1.702352    -1.447263  \n",
       "18    -1.336443    -1.635532    -1.441268  \n",
       "19    -1.286276    -1.569757    -1.396045  \n",
       "20    -1.272592    -1.481153    -1.244956  \n",
       "21    -1.270108    -1.372179    -1.227950  \n",
       "22    -1.192435    -1.365421    -1.189918  \n",
       "23    -1.153370    -1.358025    -1.177794  \n",
       "24    -1.148438    -1.242266    -1.081350  \n",
       "25    -1.141073    -1.167087    -1.007177  \n",
       "26    -1.039473    -0.926482    -0.971180  \n",
       "27    -1.030220    -0.853307    -0.945163  \n",
       "28    -0.977623    -0.713733    -0.840024  \n",
       "29    -0.976252    -0.575773    -0.753463  \n",
       "30    -0.933003    -0.542419    -0.680092  \n",
       "31    -0.790624    -0.391410    -0.674285  \n",
       "32    -0.763437    -0.250525    -0.669036  \n",
       "33    -0.652049    -0.173192    -0.540573  \n",
       "34    -0.531808     0.067298    -0.417830  \n",
       "35    -0.480700     0.482578    -0.012111  \n",
       "36          NaN          NaN          NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[-2.55256783, -0.20715416, -1.51363827],\n",
       "       [-1.28303048,  0.16792242, -3.1582522 ],\n",
       "       [-0.84371645, -2.18916157, -1.24048223],\n",
       "       [-1.54691793, -0.34099631, -2.38544602],\n",
       "       [-1.79680285, -1.66736146, -0.80919594],\n",
       "       [-0.74007854, -1.53496328, -1.99831844],\n",
       "       [-0.73632406, -3.41900085, -0.11803535],\n",
       "       [-1.49700832, -0.52075277, -2.25559916],\n",
       "       [-0.91149153, -2.33180629, -1.03006244],\n",
       "       [-1.09612591, -1.12876416, -2.04847019],\n",
       "       [-1.47718798, -1.54148001, -1.25469226],\n",
       "       [-0.22512683, -2.10263329, -1.94560013],\n",
       "       [-1.33882264, -1.25799357, -1.67654405],\n",
       "       [-0.23716719, -2.29091911, -1.74527396],\n",
       "       [-1.60790204, -1.71216659, -0.95329162],\n",
       "       [-1.12804323, -2.40679908, -0.73851795],\n",
       "       [-1.10753625, -1.30517303, -1.86065097],\n",
       "       [-1.13261103, -2.05582996, -1.08491927],\n",
       "       [-1.37945889, -0.87787175, -2.01602962],\n",
       "       [-1.55198297,  0.1541991 , -2.87557638],\n",
       "       [-2.42529819, -0.54792536, -1.30013671],\n",
       "       [-1.44250037, -1.52931837, -1.30154151],\n",
       "       [-1.90175649, -0.07405718, -2.29754659],\n",
       "       [-1.4285647 , -2.53011085, -0.3146847 ],\n",
       "       [-1.99421501, -1.77380169, -0.50534355],\n",
       "       [-2.35841652,  0.32677782, -2.24172155],\n",
       "       [-2.08962553, -0.93730082, -1.24643391],\n",
       "       [-2.4317387 ,  0.61397057, -2.45559213],\n",
       "       [-1.76713268, -0.11968226, -2.38654532],\n",
       "       [-0.97847699, -2.28594622, -1.00893705],\n",
       "       [-2.64208655, -1.00829602, -0.62297768],\n",
       "       [-0.74022406, -1.90765968, -1.62547651],\n",
       "       [-1.80836568, -0.51067835, -1.95431622],\n",
       "       [-1.40433114, -1.12258053, -1.74644859],\n",
       "       [-1.24081336, -1.03073068, -2.00181621],\n",
       "       [-1.19571831, -1.39990618, -1.67773577],\n",
       "       [-2.18627382, -1.85352122, -0.23356522],\n",
       "       [-1.63713535, -0.58248557, -2.05373933],\n",
       "       [-2.16096181, -2.45522141,  0.34282297],\n",
       "       [-2.08867134, -0.23875077, -1.94593814],\n",
       "       [-1.59302503, -1.62636646, -1.05396877],\n",
       "       [-0.33152083, -2.61177123, -1.33006819],\n",
       "       [-1.85618855, -0.98365529, -1.43351642],\n",
       "       [-1.4327006 , -0.19771141, -2.64294826],\n",
       "       [-2.63094027, -1.06295892, -0.57946107],\n",
       "       [-1.33822665, -2.7948081 , -0.1403255 ],\n",
       "       [-2.27472124,  1.80971883, -3.80835784],\n",
       "       [-0.4889361 , -1.82572644, -1.95869771],\n",
       "       [-2.0779294 ,  0.38051418, -2.57594504],\n",
       "       [-3.14580592, -0.03413467, -1.09341966],\n",
       "       [-0.75528158, -3.26183755, -0.25624113],\n",
       "       [-0.4650241 , -1.90165267, -1.90668348],\n",
       "       [-1.91120798, -0.44035155, -1.92180073],\n",
       "       [-2.96849463,  2.21524544, -3.52011107],\n",
       "       [-1.45251907, -0.73879195, -2.08204924],\n",
       "       [-2.50298092, -0.14630438, -1.62407495],\n",
       "       [-1.05965628, -0.93355012, -2.28015386],\n",
       "       [-0.78517219, -0.97829942, -2.50988865],\n",
       "       [-1.5474111 , -2.49048663, -0.23546253],\n",
       "       [-1.33756845, -2.66563732, -0.27015448]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#plot the precision recall curve, and extract the relevant decision information into a dataframe\n",
    "\n",
    "return_df=plot_precision_recall_curves_from_trained_classifier(combined_preprocessing_dict,params_dict,precision_recall_curve_timerange=[0,5],trained_clf=trained_clf,predict_validation=True)\n",
    "display(return_df)\n",
    "\n",
    "data_to_predict=preprocessing_dict['epochs'].copy().crop(tmin=0,tmax=1).get_data()[:,:]\n",
    "thresholded_prediction=trained_clf.decision_function(data_to_predict)\n",
    "\n",
    "#note that here you can decide on which thresholds to use to better optimize your \"real\" usecase\n",
    "thresholded_prediction\n",
    "#prediction=trained_clf.predict(data_to_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run on a single participant - with a single recording file (no concatination of files is needed) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration on subject: NH_Block_Default.xdf\n",
      "setting up current params: iteration 0 - grid settings: (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n",
      "running a single iteration with the following paramaters: {'LowPass': 7, 'HighPass': 32, 'PerformCsd': True, 'filter_method': 'iir', 'n_components': 4, 'n_components_fbcsp': 4, 'filters_bands': [[8, 12], [12, 20]], 'Electorde_Group': ['C5', 'C3', 'C1', 'Cz', 'C2', 'C4', 'C6', 'CP5', 'CP3', 'CP1', 'CPz', 'CP2', 'CP4', 'CP6', 'TP7', 'TP8', 'PO7', 'PO3', 'POz', 'PO4', 'PO8', 'P7', 'P5', 'P3', 'P1', 'Pz', 'P2', 'P4', 'P6', 'P8'], 'Electorde_Group_name': 'C+CP+PO+P', 'epoch_tmin': -3, 'epoch_tmax': 4, 'classifier_window_s': 1, 'classifier_window_e': 3, 'augmentation_params': {'win_len': 1, 'win_step': 0.1}, 'windowed_prediction_params': {'win_len': 0.5, 'win_step': 0.1}, 'pipeline_name': 'csp+lda', 'subject': 'NH', 'recording_file': 'NH_Block_Default.xdf'}\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "file c:\\Users\\gilad\\MI-VR_Project\\Recordings\\NH_Block_Default.xdf does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 33\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mrunning a single iteration with the following paramaters:\u001b[39m\u001b[39m'\u001b[39m,params_dict)\n\u001b[0;32m     32\u001b[0m \u001b[39m#test it: \u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m train_inds,validation_inds,preprocessing_dict\u001b[39m=\u001b[39mrun_pre_processing_extract_validation_set(recording_path,current_path,params_dict)\n\u001b[0;32m     34\u001b[0m params_dict[\u001b[39m'\u001b[39m\u001b[39mpreprocessing_dict\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m=\u001b[39mpreprocessing_dict\n\u001b[0;32m     35\u001b[0m fig,w_times,scores_windows,folds_confusion_metrices_per_window,validation_scores,validation_confusion_metrices_per_window,trained_clf\u001b[39m=\u001b[39mrun_training_and_classification_on_selected_params(params_dict,preprocessing_dict,to_plot\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,figure_outputs_path\u001b[39m=\u001b[39mfigure_outputs_path,fig_name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[6], line 15\u001b[0m, in \u001b[0;36mrun_pre_processing_extract_validation_set\u001b[1;34m(recording_path, current_path, params_dict)\u001b[0m\n\u001b[0;32m     12\u001b[0m events_trigger_dict\u001b[39m=\u001b[39m{val:key \u001b[39mfor\u001b[39;00m key,val \u001b[39min\u001b[39;00m events_trigger_dict\u001b[39m.\u001b[39mitems()}\n\u001b[0;32m     14\u001b[0m \u001b[39m#read the file:\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m Raw\u001b[39m=\u001b[39mread_raw_xdf(recording_path \u001b[39m/\u001b[39;49m params_dict[\u001b[39m'\u001b[39;49m\u001b[39mrecording_file\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m     16\u001b[0m \u001b[39m#remove non existent channels: \u001b[39;00m\n\u001b[0;32m     17\u001b[0m Raw\u001b[39m.\u001b[39mdrop_channels([\u001b[39m'\u001b[39m\u001b[39mACC_X\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mACC_Y\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mACC_Z\u001b[39m\u001b[39m'\u001b[39m]) \u001b[39m## Drop non eeg channels\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\gilad\\MI-VR_Project\\mne_import_xdf.py:166\u001b[0m, in \u001b[0;36mread_raw_xdf\u001b[1;34m(fname, stream_id)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mread_raw_xdf\u001b[39m(fname, stream_id\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    151\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Read XDF file.\u001b[39;00m\n\u001b[0;32m    152\u001b[0m \n\u001b[0;32m    153\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[39m        XDF file data.\u001b[39;00m\n\u001b[0;32m    165\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 166\u001b[0m     streams, header \u001b[39m=\u001b[39m load_xdf(fname)\n\u001b[0;32m    168\u001b[0m     \u001b[39mif\u001b[39;00m stream_id \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    169\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(stream_id, \u001b[39mstr\u001b[39m):\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\BCIEnvironment\\lib\\site-packages\\pyxdf\\pyxdf.py:231\u001b[0m, in \u001b[0;36mload_xdf\u001b[1;34m(filename, select_streams, on_chunk, synchronize_clocks, handle_clock_resets, dejitter_timestamps, jitter_break_threshold_seconds, jitter_break_threshold_samples, clock_reset_threshold_seconds, clock_reset_threshold_stds, clock_reset_threshold_offset_seconds, clock_reset_threshold_offset_stds, winsor_threshold, verbose)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[39m# XML content of the file header chunk\u001b[39;00m\n\u001b[0;32m    229\u001b[0m fileheader \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 231\u001b[0m \u001b[39mwith\u001b[39;00m open_xdf(filename) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m    232\u001b[0m     \u001b[39m# for each chunk\u001b[39;00m\n\u001b[0;32m    233\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    234\u001b[0m         \u001b[39m# noinspection PyBroadException\u001b[39;00m\n\u001b[0;32m    235\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    236\u001b[0m             \u001b[39m# read [NumLengthBytes], [Length]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\BCIEnvironment\\lib\\site-packages\\pyxdf\\pyxdf.py:420\u001b[0m, in \u001b[0;36mopen_xdf\u001b[1;34m(file)\u001b[0m\n\u001b[0;32m    418\u001b[0m \u001b[39m# check absolute path after following symlinks\u001b[39;00m\n\u001b[0;32m    419\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m filename\u001b[39m.\u001b[39mresolve()\u001b[39m.\u001b[39mexists():\n\u001b[1;32m--> 420\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mfile \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m does not exist.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m filename)\n\u001b[0;32m    422\u001b[0m \u001b[39mif\u001b[39;00m filename\u001b[39m.\u001b[39msuffix \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m.xdfz\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m filename\u001b[39m.\u001b[39msuffixes \u001b[39m==\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39m.xdf\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m.gz\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m    423\u001b[0m     f \u001b[39m=\u001b[39m gzip\u001b[39m.\u001b[39mopen(\u001b[39mstr\u001b[39m(filename), \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mException\u001b[0m: file c:\\Users\\gilad\\MI-VR_Project\\Recordings\\NH_Block_Default.xdf does not exist."
     ]
    }
   ],
   "source": [
    "#cell is suitable for the old participants with a single recording file - dont use on roi! \n",
    "#uncomment the \"\"\" to use it: \n",
    "\n",
    "\n",
    "#define which subject to currently check: (here you can also try to define the filename explicitly, if you dont want to work with the \"recording files list that is created earlier\")\n",
    "recording_file=recording_files[6]\n",
    "Subject=subject_names[6]\n",
    "\n",
    "print(f'iteration on subject: {recording_file}')\n",
    "#put all in a single params_dictionary for the current run: \n",
    "\n",
    "#get all possible grid_search combinations: \n",
    "grid_search_dict_copy=grid_search_dict.copy()\n",
    "all_grid_combinations = list(itertools.product(*all_options))\n",
    "iteration_ind=0 #select some grid search combination - you can manualy change the params after getting the \"params_dict\" below\n",
    "\n",
    "#here i can change manually the current iteration params: \n",
    "grid_search_dict_copy['Electorde_Groups_names_grid']=['C+CP+PO+P']\n",
    "grid_search_dict_copy['filters_bands']=[[[8,12],[12,20]]]#[[[8,12], [12, 16],[16,20],[20,24],[24,28],[28,32]]]\n",
    "#this cell allow to test specific iterations\n",
    "params_dict=set_up_params_for_current_grid_iteration(all_grid_combinations,iteration_ind,grid_search_dict_copy)\n",
    "#remember - you can change param_dict manualy - ALL of its keys, so before examining your iteration, make sure that you manually adressed all the relevant keys to change. \n",
    "params_dict['subject']=Subject\n",
    "params_dict['recording_file']=recording_file\n",
    "params_dict['augmentation_params']={'win_len': 1, 'win_step': 0.1}\n",
    "params_dict['classifier_window_s']=1\n",
    "params_dict['classifier_window_e']=3\n",
    "params_dict['pipeline_name']='csp+lda'\n",
    "params_dict['n_components_fbcsp']=4\n",
    "print('running a single iteration with the following paramaters:',params_dict)\n",
    "\n",
    "#test it: \n",
    "train_inds,validation_inds,preprocessing_dict=run_pre_processing_extract_validation_set(recording_path,current_path,params_dict)\n",
    "params_dict['preprocessing_dict']=preprocessing_dict\n",
    "fig,w_times,scores_windows,folds_confusion_metrices_per_window,validation_scores,validation_confusion_metrices_per_window,trained_clf=run_training_and_classification_on_selected_params(params_dict,preprocessing_dict,to_plot=True,figure_outputs_path=figure_outputs_path,fig_name='test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example of how to plot a confusion  matrix from the first fold (training set) on the first window\n",
    "disp=ConfusionMatrixDisplay(folds_confusion_metrices_per_window[1][0],display_labels=trained_clf.classes_)\n",
    "disp.plot()\n",
    "plt.show()\n",
    "\n",
    "disp=ConfusionMatrixDisplay(np.array(validation_confusion_metrices_per_window).mean(axis=0),display_labels=trained_clf.classes_)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chosen window prediction range is [1, 2]\n",
      "note that the prediction paramaters (that the classifier is trained on) are: {'win_len': 0.5, 'win_step': 0.1}\n",
      "consider if you want the preciction range to match the prediction_param\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 158 is out of bounds for axis 0 with size 60",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m#plot and extract relevfant decision information for the precision recall curve - this time suited to 2 classes (binary classification) - which basicly means that the output score is single (and not 2 scores are you  might have expected)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m return_df\u001b[39m=\u001b[39mplot_precision_recall_curves_from_trained_classifier(preprocessing_dict,params_dict,precision_recall_curve_timerange\u001b[39m=\u001b[39;49m[\u001b[39m1\u001b[39;49m,\u001b[39m2\u001b[39;49m],trained_clf\u001b[39m=\u001b[39;49mtrained_clf,predict_validation\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m      3\u001b[0m display(return_df)\n\u001b[0;32m      5\u001b[0m data_to_predict\u001b[39m=\u001b[39mpreprocessing_dict[\u001b[39m'\u001b[39m\u001b[39mepochs\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mcopy()\u001b[39m.\u001b[39mcrop(tmin\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m,tmax\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m)\u001b[39m.\u001b[39mget_data()[:,:]\n",
      "Cell \u001b[1;32mIn[10], line 22\u001b[0m, in \u001b[0;36mplot_precision_recall_curves_from_trained_classifier\u001b[1;34m(preprocessing_dict, params_dict, precision_recall_curve_timerange, trained_clf, predict_validation)\u001b[0m\n\u001b[0;32m     18\u001b[0m     inds\u001b[39m=\u001b[39mpreprocessing_dict[\u001b[39m'\u001b[39m\u001b[39mtrain_inds\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     21\u001b[0m \u001b[39m#extract the labels: \u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m labels\u001b[39m=\u001b[39mpreprocessing_dict[\u001b[39m'\u001b[39;49m\u001b[39mepochs\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mevents[inds, \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]\n\u001b[0;32m     23\u001b[0m \u001b[39m#extract the decision function: \u001b[39;00m\n\u001b[0;32m     24\u001b[0m decision_function\u001b[39m=\u001b[39mtrained_clf\u001b[39m.\u001b[39mdecision_function(preprocessing_dict[\u001b[39m'\u001b[39m\u001b[39mepochs\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mcopy()\u001b[39m.\u001b[39mcrop(tmin\u001b[39m=\u001b[39mprecision_recall_curve_timerange[\u001b[39m0\u001b[39m],tmax\u001b[39m=\u001b[39mprecision_recall_curve_timerange[\u001b[39m1\u001b[39m])\u001b[39m.\u001b[39mget_data()[inds,:])\n",
      "\u001b[1;31mIndexError\u001b[0m: index 158 is out of bounds for axis 0 with size 60"
     ]
    }
   ],
   "source": [
    "#plot and extract relevfant decision information for the precision recall curve - this time suited to 2 classes (binary classification) - which basicly means that the output score is single (and not 2 scores are you  might have expected)\n",
    "return_df=plot_precision_recall_curves_from_trained_classifier(preprocessing_dict,params_dict,precision_recall_curve_timerange=[1,2],trained_clf=trained_clf,predict_validation=True)\n",
    "display(return_df)\n",
    "\n",
    "data_to_predict=preprocessing_dict['epochs'].copy().crop(tmin=0,tmax=4).get_data()[:,:]\n",
    "thresholded_prediction=trained_clf.decision_function(data_to_predict)<0.520642 #example of how to use a custom threshold you extracted from the return_df\n",
    "prediction=trained_clf.predict(data_to_predict)\n",
    "\n",
    "#show the difference between the custom and the default prediction thresholds.  \n",
    "pd.DataFrame({'thresholded':thresholded_prediction,'default':prediction})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## this section runs custom grid search for neta: \n",
    "### didnt update this code to work with the combined_preprocessing_structure of roi (shouldnt be too much changes needed)\n",
    "#### defines a custom grid \n",
    "#### define the participant name and file \n",
    "#### run the grid search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid options [[0], [0], [0, 1], [0, 1], [0, 1, 2, 3], [0, 1], [0], [0], [0], [0, 1], [0, 1], [0]]\n",
      "number of grid search iterations: 128\n"
     ]
    }
   ],
   "source": [
    "#simple named groups: \n",
    "Electorde_Groups = {'FP': ['Fp1', 'Fp2'],\n",
    "                   'AF': ['AF7', 'AF3', 'AFz', 'AF4', 'AF8'],\n",
    "                   'F' : ['F7', 'F5', 'F3', 'F1', 'Fz', 'F2', 'F4', 'F6', 'F8'],\n",
    "                   'FC': ['FC5', 'FC3', 'FC1', 'FC2', 'FC4', 'FC6', 'FT7','FT8'],\n",
    "                   'C' : ['C5', 'C3', 'C1', 'Cz', 'C2', 'C4' ,'C6'],\n",
    "                   'CP': ['CP5', 'CP3', 'CP1','CPz', 'CP2', 'CP4', 'CP6','TP7', 'TP8'],\n",
    "                   'P' : ['P7', 'P5','P3', 'P1', 'Pz', 'P2', 'P4', 'P6', 'P8'],\n",
    "                   'PO': ['PO7', 'PO3', 'POz', 'PO4', 'PO8'],\n",
    "                   'O' : ['Oz', 'O2', 'O1', 'Iz']\n",
    "                  } \n",
    "#set up custom group names: \n",
    "Electorde_Groups = {'Motor_cortex':['F3', 'F1', 'Fz', 'F2', 'F4']+['FC3', 'FC1', 'FC2', 'FC4']+['C3', 'C1', 'Cz', 'C2', 'C4']+\n",
    "                    ['CP3', 'CP1','CPz', 'CP2', 'CP4']+['P3', 'P1', 'Pz', 'P2', 'P4'],\n",
    "                    'Motor_cortex_extended': ['F3', 'F1', 'Fz', 'F2', 'F4']+['FC3', 'FC1', 'FC2', 'FC4']+['C5','C3', 'C1', 'Cz', 'C2', 'C4','C6']+\n",
    "                    ['CP3', 'CP1','CPz', 'CP2', 'CP4']+['P3', 'P1', 'Pz', 'P2', 'P4'],\n",
    "                    'Visual_cortex': ['P7', 'P5','P3', 'P1', 'Pz', 'P2', 'P4', 'P6', 'P8']+['PO7', 'PO3', 'POz', 'PO4', 'PO8']+['Oz', 'O2', 'O1', 'Iz'],\n",
    "                    'all':  ['Fp1', 'Fp2']+['AF7', 'AF3', 'AFz', 'AF4', 'AF8']+ ['F7', 'F5', 'F3', 'F1', 'Fz', 'F2', 'F4', 'F6', 'F8']+\n",
    "                    ['FC5', 'FC3', 'FC1', 'FC2', 'FC4', 'FC6', 'FT7','FT8']+['C5', 'C3', 'C1', 'Cz', 'C2', 'C4' ,'C6']+['CP5', 'CP3', 'CP1','CPz', 'CP2', 'CP4', 'CP6','TP7', 'TP8']+\n",
    "                    ['P7', 'P5','P3', 'P1', 'Pz', 'P2', 'P4', 'P6', 'P8']+['PO7', 'PO3', 'POz', 'PO4', 'PO8']+['Oz', 'O2', 'O1', 'Iz'],\n",
    "                    'Motor_cortex_extended_po': ['F3', 'F1', 'Fz', 'F2', 'F4']+['FC3', 'FC1', 'FC2', 'FC4']+['C5','C3', 'C1', 'Cz', 'C2', 'C4','C6']+\n",
    "                    ['CP3', 'CP1','CPz', 'CP2', 'CP4']+['P3', 'P1', 'Pz', 'P2', 'P4']+['PO3', 'POz', 'PO4']}\n",
    "\n",
    "\n",
    "\n",
    "#set the current search: \n",
    "grid_search_dict=OrderedDict()\n",
    "grid_search_dict={'filter_methods':['iir'], #['irr' or 'fir']\n",
    "                'run_csd':[True],\n",
    "                'pipeline_name':['csp+lda','ts+lda'],#'fbcsp+lda'], #these classifiers pipelines are defined in \"run_windowed_classification_on_fold\"\n",
    "                #things to do: filter bank csp + lda, csp+ts+lda\n",
    "                'bandpass_borders_grid':[[7,32],[8,30]], #each list defines the low and high cutoffs\n",
    "                'Electorde_Groups_names_grid':['Motor_cortex','Motor_cortex_extended','Motor_cortex_extended_po','all'], #each \"name\" refers to an elec group defined above\n",
    "                'n_components_grid':[4,6], #the n component options for the csp classifier\n",
    "                'n_components_fbcsp_grid':[2], # the n components options to use in the fbcsp classifier (n * filter_bank_bands)\\\n",
    "                'filters_bands':[[[8, 12], [12, 20], [20, 32]]],\n",
    "                'epoch_tmins_and_maxes_grid':[[-3,4]], #times (sec: pre,post) for initial epoching (this should be the longest epoch as the windowed prediction will be tested on it)\n",
    "                'classifier_training_windows_grid':[[0.5 , 3.5],[1,4]], #what times(sec: start,end) to use for the classifer training (data augmentation is also using this window)\n",
    "                'augmentation_windows_grid':[[1,0.2],[1,0.1]], #referes to proportions (win_len,win_step) of sfreq, [1,1] means taking the classification epochs, and creating 1 second long epochs with 1 second long steps\n",
    "                'windowed_prediction_params':[[1,0.1]]} #refers to prportions (win_len,win_step) of sfreq to try and predict i.e. 0.5 = half a second window, with a 100ms steps  \n",
    "\n",
    "all_options=[list(range(len(val))) for key,val in grid_search_dict.items()]\n",
    "print(f'grid options {all_options}')\n",
    "#get all possible grid_search combinations: \n",
    "all_grid_combinations = list(itertools.product(*all_options))\n",
    "print(f'number of grid search iterations: {len(all_grid_combinations)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run a grid search on a single participant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Subject_name='Neta'\n",
    "recording_file_name='Neta_NoAO_1Hand.xdf'\n",
    "grid_search_data_frame_info=run_grid_search_on_single_participant(grid_search_dict,recording_file_name,Subject_name,save_every_n_iter=5,save_location_path=hyper_param_search_output,to_plot=False)\n",
    "display(grid_search_data_frame_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_data_frame_info"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run the entire grid search (on all participants): save the outcomes in a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for recording_file,Subject in tqdm(zip(recording_files,subject_names)): #run on all participants: \n",
    "    Subject_name=Subject\n",
    "    recording_file_name=recording_file\n",
    "    grid_search_data_frame_info=run_grid_search_on_single_participant(grid_search_dict,recording_file_name,Subject_name,save_every_n_iter=5,save_location_path=hyper_param_search_output,to_plot=True)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## take all the rest of the code from here: \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the cells below load up the grid search to look for the best hyper paramaters: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WindowsPath('c:/Users/d_abe/Desktop/mental imagery project/MI-VR_Project/hyper_param_search_outputs/best_hyper_params_max_prediction.csv'), WindowsPath('c:/Users/d_abe/Desktop/mental imagery project/MI-VR_Project/hyper_param_search_outputs/best_hyper_params_mean_prediction.csv'), WindowsPath('c:/Users/d_abe/Desktop/mental imagery project/MI-VR_Project/hyper_param_search_outputs/hypter_param_search_Dekel_AO.csv'), WindowsPath('c:/Users/d_abe/Desktop/mental imagery project/MI-VR_Project/hyper_param_search_outputs/hypter_param_search_Dekel_AoNoMI.csv'), WindowsPath('c:/Users/d_abe/Desktop/mental imagery project/MI-VR_Project/hyper_param_search_outputs/hypter_param_search_Dekel_NoAO.csv'), WindowsPath('c:/Users/d_abe/Desktop/mental imagery project/MI-VR_Project/hyper_param_search_outputs/hypter_param_search_Gilad_AO.csv'), WindowsPath('c:/Users/d_abe/Desktop/mental imagery project/MI-VR_Project/hyper_param_search_outputs/hypter_param_search_Neta_AO_1Hand.csv'), WindowsPath('c:/Users/d_abe/Desktop/mental imagery project/MI-VR_Project/hyper_param_search_outputs/hypter_param_search_Neta_AO_2Hands.csv'), WindowsPath('c:/Users/d_abe/Desktop/mental imagery project/MI-VR_Project/hyper_param_search_outputs/hypter_param_search_Neta_NoAO_1Hand.csv'), WindowsPath('c:/Users/d_abe/Desktop/mental imagery project/MI-VR_Project/hyper_param_search_outputs/hypter_param_search_Neta_NoAO_2Hands.csv')]\n"
     ]
    }
   ],
   "source": [
    "subject_specific_hyper_param_csvs=[curr_csv for curr_csv in hyper_param_search_output.iterdir() if (('.csv' in curr_csv.name))]\n",
    "print(subject_specific_hyper_param_csvs)\n",
    "\n",
    "#read the saved dataframes: also note that when cells with np array were saved, they are read out as strings so we need to convert them back to np arrays for easy of use\n",
    "def convert_saved_string_to_np_array(string):\n",
    "    array_format=np.array([float(num) for num in string.replace('[','').replace(']','').replace('\\n','').split(' ') if len(num)>0])\n",
    "    return array_format\n",
    "#this section loads and concatinate all hyper params searchs into a single df\n",
    "subjects_df_all=pd.concat([pd.read_csv(curr_csv,index_col=0,dtype={ 'mean_scores':object,\n",
    "       'std_scores':object, 'w_times':object,'validation_scores':object},converters={ 'mean_scores':convert_saved_string_to_np_array,\n",
    "       'std_scores':convert_saved_string_to_np_array, 'w_times':convert_saved_string_to_np_array,'validation_scores':convert_saved_string_to_np_array}) for curr_csv in subject_specific_hyper_param_csvs],axis=0)\n",
    "subjects_df_all.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this section create functions used to infer the w_times vector (the time stamp of the middle of a prediction window) based on the\n",
    "#prediction paramaters in the dataframe\n",
    "\n",
    "def isfloat(num):\n",
    "    try:\n",
    "        float(num)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "    \n",
    "def calculate_w_times(row):\n",
    "        min_time = row['epoch_tmin']\n",
    "        max_time = row['epoch_tmax']\n",
    "        pred_win_string = row['windowed_prediction_params'].replace('{','').replace('}','').replace(',','')\n",
    "        pred_win_len = [float(x) for x in pred_win_string.split(':')[1].split(' ') if isfloat(x)][0]\n",
    "        pred_win_step = [float(x) for x in pred_win_string.split(':')[2].split(' ') if isfloat(x)][0]\n",
    "        first_window = min_time \n",
    "        last_window = max_time - pred_win_len \n",
    "        w_times = np.linspace(first_window,last_window,row['mean_scores'].shape[0])\n",
    "        assert(row['mean_scores'].shape==w_times.shape)\n",
    "        return w_times\n",
    "\n",
    "#this is where we define a function to search for the best performence over a requested window (prediction window defined by the timestamp of its start)\n",
    "def get_performence_in_time_window(subjects_df_all,time_window,operation='mean'):\n",
    "    #time window defineds the time stamp of the start of the window\n",
    "    #possible operations: \"mean\",'max','min'\n",
    "    \n",
    "    values=subjects_df_all['mean_scores'].values\n",
    "    times=subjects_df_all['w_times'].values\n",
    "    extracted_values=[]\n",
    "    for curr_times,curr_values in zip(times,values): \n",
    "        rel_values=curr_values[(curr_times>time_window[0]) & (curr_times<=time_window[1])]\n",
    "        if operation=='mean':\n",
    "            val=np.mean(rel_values)\n",
    "        elif operation=='max':\n",
    "            val=np.max(rel_values)\n",
    "        elif operation=='min':\n",
    "            val=np.min(rel_values)    \n",
    "        extracted_values.append(val)\n",
    "    return extracted_values\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      max_1_4         mean_1_4       \n",
      "                          max idxmax       max idxmax\n",
      "recording_file                                       \n",
      "Dekel_AO.xdf          0.68750      0  0.653750      8\n",
      "Dekel_AoNoMI.xdf      0.68750      1  0.585000      9\n",
      "Dekel_NoAO.xdf        0.74375      2  0.685312      2\n",
      "Gilad_AO.xdf          0.88750      3  0.789687     11\n",
      "Neta_AO_1Hand.xdf     0.68750      4  0.597187     12\n",
      "Neta_AO_2Hands.xdf    0.76875      5  0.669063     13\n",
      "Neta_NoAO_1Hand.xdf   0.71250      6  0.572283     14\n",
      "Neta_NoAO_2Hands.xdf  0.66250      7  0.536875     15\n"
     ]
    }
   ],
   "source": [
    "#infer w_times\n",
    "subjects_df_all['w_times'] = subjects_df_all.apply(calculate_w_times, axis=1)\n",
    "time_window=[1,4]\n",
    "max_col_name='max'+'_'+str(time_window[0])+'_'+str(time_window[1])\n",
    "mean_col_name='mean'+'_'+str(time_window[0])+'_'+str(time_window[1])\n",
    "subjects_df_all[max_col_name]= get_performence_in_time_window(subjects_df_all,time_window,operation='max') \n",
    "subjects_df_all[mean_col_name]= get_performence_in_time_window(subjects_df_all,time_window,operation='mean') \n",
    "grouped_df=subjects_df_all.groupby(['recording_file']).aggregate({max_col_name:['max','idxmax'],mean_col_name:['max','idxmax']})\n",
    "print(grouped_df)\n",
    "#best hyperparams per subject: \n",
    "best_hyper_params_df_mean_prediction=subjects_df_all.iloc[grouped_df[(mean_col_name,'idxmax')].values]\n",
    "best_hyper_params_df_mean_prediction.to_csv(hyper_param_search_output/'best_hyper_params_mean_prediction.csv',index=False)\n",
    "best_hyper_params_df_max_prediction=subjects_df_all.iloc[grouped_df[(max_col_name,'idxmax')].values]\n",
    "best_hyper_params_df_max_prediction.to_csv(hyper_param_search_output/'best_hyper_params_max_prediction.csv',index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a function to take the \"best_hyper_params_df_max_prediction\" and the required index to plot the results\n",
    "def plot_prediction_time_course_from_hyper_param_search(hyper_param_seach_df,ind):\n",
    "    plot_df=pd.DataFrame({'w_times':hyper_param_seach_df.loc[ind]['w_times'],\n",
    "                        'mean_scores':hyper_param_seach_df.loc[ind]['mean_scores'],\n",
    "                        'std_scores':hyper_param_seach_df.loc[ind]['std_scores'],\n",
    "                        'validation_scores':hyper_param_seach_df.loc[ind]['validation_scores']})\n",
    "    fig=plt.figure()\n",
    "    plt.fill_between(plot_df['w_times'],plot_df['mean_scores']+plot_df['std_scores'],plot_df['mean_scores']-plot_df['std_scores'],alpha=0.25,label=' +-1 std')\n",
    "    sns.lineplot(data=plot_df,x='w_times',y='mean_scores',label='mean cv score',color='g')\n",
    "    sns.lineplot(data=plot_df,x='w_times',y='validation_scores',color='r',label='validation set score')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.grid()\n",
    "    plt.axhline(y=0.5, color='k', linestyle='--')\n",
    "    plt.axvline(x=0,color='k', linestyle='-.')\n",
    "\n",
    "    #get info for title: \n",
    "    filename=hyper_param_seach_df.loc[ind]['recording_file']\n",
    "    max_col_name=[col for col in hyper_param_seach_df.columns if 'max_' in col]\n",
    "    mean_col_name=[col for col in hyper_param_seach_df.columns if (('mean_' in col) and ('scores' not in col))]\n",
    "\n",
    "    max_value=hyper_param_seach_df.loc[ind][max_col_name].values\n",
    "    mean_value=hyper_param_seach_df.loc[ind][mean_col_name].values\n",
    "    prediction_win_params=hyper_param_seach_df.loc[ind]['windowed_prediction_params']\n",
    "    pred_win_string = prediction_win_params.replace('{','').replace('}','').replace(',','')\n",
    "    pred_win_len = [float(x) for x in pred_win_string.split(':')[1].split(' ') if isfloat(x)][0]\n",
    "    pred_win_step = [float(x) for x in pred_win_string.split(':')[2].split(' ') if isfloat(x)][0]\n",
    "\n",
    "    title=f'{filename}\\nwin_size = {pred_win_len}, step_size={pred_win_step}\\ncv scores:\\n{max_col_name}={max_value},{mean_col_name}={mean_value}'\n",
    "    plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing it (using the(3rd row - gilad AO, as it looks awesome! :P)) \n",
    "plot_prediction_time_course_from_hyper_param_search(best_hyper_params_df_mean_prediction,best_hyper_params_df_mean_prediction.index[3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Mental_Imagery",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8dd1d82d0489786582d665b69714801e3e965ab3972c8c510c769fb6057b2a16"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
